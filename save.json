{"schema_version":1,"items":[{"name":"My Notes","id":"e04ba4d4-41ce-46cb-b5ee-5ec14a7ca79e","color":"#000000","icon":"book-2","children":[{"name":"README","id":"4ba4d441-ce36-4b35-ae5e-c14a7ca79e6a","color":"#999999","icon":"file-text","fileName":"Example-Note4ba4d441-ce36-4b","textContent":"Contents\n\nBacktracking\n\nBacktracking\n\nAlways sketch out the recursion tree in backtracking problems, solving becomes easy from there\n\nString Partitioning\n\nPartition a string into all possible substrings\n\nUses recursive calls to partition and check whatever valid condition needs to be checked (eg. is a palindrome, is present in a dictionary etc.)\n\nExample Problems \n\nProblem\n\nLink\n\nNotes\n\nPalindrome Partitioning\n\nhttps://leetcode.com/problems/palindrome-partitioning/description/\n\ncondition is palindrome\n\nWord Break II\n\nhttps://leetcode.com/problems/word-break-ii/description/\n\ncondition is present in dictionary\n\nresult = []\nn = len(s)\n\ndef backtrack(curr_partition, start):\n    if start == n:\n        result.append(curr_partition)\n        return\n    \n    for end in range(start + 1, n + 1):\n        substring = s[start:end]\n        if is_valid_substring(substring): #is_valid_substring changes with the type of problem at hand\n            curr_partition.append(substring)\n            backtrack(curr_partition, end) #Can also use backtrack(curr_partition + [substring], end)\n            curr_partition.pop()\n\nbacktrack([], 0)\nreturn result\n\ns: The input string to be partitioned.\n\nis_valid_substring: A function that checks if a given substring is valid (e.g., checks if it is a palindrome or if it is in a dictionary).\n\nresult: A list to store all the valid partitions.\n\nbacktrack Function: A helper function that performs the actual backtracking.\n\ncurr_partition: The current partition being constructed.\n\nstart: The starting index for partitioning the string.\n\nBase Case: If start reaches the end of the string (n), we add the current partition to the result.\n\nRecursive Case: We iterate through possible end indices (end), generate substrings from start to end, and check if they are valid using the is_valid_substring function. If valid, we recursively call backtrack with the new substring added to the current partition. After the recursive call, we backtrack by removing the last added substring.\n\nSubsets\n\nRecursively generate all subsets from a given set\n\nCan use 2 principles, 1st is for loop (so subsets starting with element) and 2nd is inclusion exclusion (Add curr to result only at the end)\n\nIt is better to use inclusion/exclusion when you want all subsets without any restrictions (combination sum, original subsets problem etc.), otherwise when you have restrictions (length, no duplicates etc.), it is better to use for loop strategy\n\nCan slightly modify the logic on what subsets to include based on any conditions (duplicates etc.)\n\nExample problems\n\nProblem\n\nLink\n\nNotes\n\nSubsets\n\nhttps://leetcode.com/problems/subsets/description/\n\nSubsets II\n\nhttps://leetcode.com/problems/subsets-ii/description/\n\nNo Duplicate Subsets\n\nCombinations\n\nhttps://leetcode.com/problems/combinations/description/\n\nSubsets of length k\n\nCombination Sum\n\nhttps://leetcode.com/problems/combination-sum/description/\n\nRepetition Allowed\n\nCombination Sum II\n\nhttps://leetcode.com/problems/combination-sum-ii/description/\n\nNo Repetition \n\nresult = []\nn = len(nums)\n\ndef backtrack(curr_subset, start):\n    result.append(list(curr_subset))\n    \n    for i in range(start, n):\n        curr_subset.append(nums[i]) # Add the current subset to the result\n        backtrack(curr_subset, i + 1) # Move to the next element\n        curr_subset.pop() # Exclude the current element (backtrack)\n\nbacktrack([], 0)\nreturn result\n\nresult = []\nn= = len(nums)\n\ndef backtrack(curr, i):\n    if i == n: # Base case: if we've considered all elements\n        result.append(curr)  \n        return\n\n    dfs(curr + [nums[i]], i + 1) # Inclusion: include nums[i] in the subset\n    dfs(curr, i + 1) # Exclusion: exclude nums[i] from the subset\n\nbacktrack([], 0)\nreturn result\n\nnums: The input list of numbers from which to generate subsets.\n\nresult: A list to store all the subsets.\n\nbacktrack Function: A helper function that performs the actual backtracking.\n\ncurr_subset: The current subset being constructed.\n\nstart: The starting index for the next element to consider.\n\nBase Case: Every time we call backtrack, we add the current subset (curr_subset) to the result.\n\nRecursive Case: We iterate through the elements starting from start to n, include the current element in the subset, and recursively call backtrack with the next starting index. After the recursive call, we backtrack by removing the last added element to explore other subsets.\n\nCode to identify non repeating elements in an array\n\nprev = None\n\nfor i in range(n):\n    if arr[i] != prev:\n        # Do something with the unique element 'arr[i]'\n        print(arr[i])  # Replace this with your desired operation\n    prev = arr[i]\n\n# Or easier, just use continue statement, if arr[i]==arr[i-1]: continue\n\nPermutations\n\nJust a slight variation of the combinations problem. Permutations generate all possible orders of elements in a given set. \n\nThe idea is to explore every possible order by fixing one element at a time and recursively permuting the remaining elements.\n\nExample problems:\n\nProblem\n\nLink\n\nNotes\n\nPermutations\n\nhttps://leetcode.com/problems/permutations/description/\n\nPermutations II\n\nhttps://leetcode.com/problems/permutations-ii/\n\nNo Duplicates\n\nresult = []\nn = len(nums)\n\ndef backtrack(curr_permutation, used):\n    if len(curr_permutation) == n: # Base case: if the current permutation is of length n, add it to result\n        result.append(list(curr_permutation))\n        return\n    \n    for i in range(n): \n        if used[i]: # Skip already used elements\n            continue\n\n        used[i] = True # Mark the current element as used\n        curr_permutation.append(nums[i]) \n        backtrack(curr_permutation, used) # Recurse with the updated permutation and used status\n        used[i] = False # Backtrack: unmark the element and remove it from the current permutation\n        curr_permutation.pop() \n\nbacktrack([], [False] * n)\nreturn result\n\n#You can maintain used, or just directly check if nums[i] is present in curr (this is O(n)), and if yes, just skip it. \n\nnums: The input list of numbers for which we want to generate permutations.\n\nresult: A list to store all the permutations.\n\nbacktrack Function: A helper function to perform the actual backtracking.\n\ncurr_permutation: The current permutation being constructed.\n\nused: A boolean list to keep track of which elements are used in the current permutation.\n\nBase Case: If the length of curr_permutation is equal to n, the current permutation is added to the result.\n\nRecursive Case: Iterate through the elements, check if the current element is used, and recursively generate permutations with the rest of the elements. After the recursive call, backtrack by marking the element as unused and removing it from the permutation.\n\nConstructing Valid Configurations\n\nConstruct valid solutions that adhere to specific constraints (like placing elements on a grid)\n\nExample Problems:\n\nProblem\n\nLink\n\nNotes\n\nN-Queens\n\nhttps://leetcode.com/problems/n-queens/description/\n\nSudoku Solver\n\nhttps://leetcode.com/problems/sudoku-solver/description/\n\ndef is_valid(inputs):\n    #Check the validity (n-queens is valid, sudoku board is valid etc.)\ndef backtrack(inputs):\n    \n    #the outer loops can changem this template is not 100% fitting, just for idea\n    #for loop (whatever you want to loop on)\n        if is_valid(inputs):\n            board[row][col] = 'Q'  # Place queen (In case of sudoku, its number)\n            #backtrack\n            board[row][col] = '.'  # Backtrack (remove queen/number)\n\nbacktrack(inputs)\nreturn result\n\nNote: Have to write code templates for dynamic programming + backtracking, DFS/BFS + backtracking. Mostly will be covered in DP and graph subsections. \n\nNote: There's also problems like valid parentheses, that don't fall into any of the patterns listed above, but it's just general backtracking and knowing when to stop (opening brackets >= closing brackets)\n\nGraphs\n\nMost leetcode problems in graphs are either adjacency lists, or grid based problems. Occasionally, you might encounter graphs represented using Nodes, and graphs represented using adjacency matrix. \n\nTry to pass both the graph, visited set as arguments to the function, as in Python, only references to mutable objects are passed, so it is the same object, not a copy. \n\nConverting edges to adjacency list\n\ndef edge_list_to_adj_list(edges: list, n: int):\n    # Create an empty adjacency list with default as an empty list\n    adj_list = defaultdict(list)\n    \n    # Iterate over the edge list to populate the adjacency list\n    for u, v in edges:\n        adj_list[u].append(v)\n        adj_list[v].append(u)  # If the graph is undirected, add both ways\n    \n    return adj_list\n\nDFS vs BFS\n\nDFS:\n\nMark node as visited: When popped from the stack (ready to process). \n\nWhy?: Ensures full exploration of neighbors before marking as visited.\n\nAdditional visited check: Needed before pushing neighbors onto the stack to avoid pushing the same node multiple times (because DFS might revisit nodes from different branches). (Only in case of iterative)\n\nRecursive DFS:\n\nNo need for an additional visited check because the recursion stack inherently manages depth-first traversal, and nodes are marked as visited immediately when the function is called.\n\nThe call stack prevents revisiting nodes by the nature of recursion.\n\nIn recursive DFS, you process the node first, recursively call neighbors, and only after all recursive calls are done does the node \"pop\" from the stack (when the function returns).\n\nIterative DFS:\n\nRequires two visited checks:\n\nBefore pushing neighbors onto the stack, to avoid pushing already visited nodes.\n\nBefore processing the node after popping from the stack, to ensure the node is only processed once, even if it's added to the stack multiple times from different paths.\n\nIn iterative DFS, you pop the node first, process it, then push neighbors to the stack.\n\nThe difference arises from how the call stack in recursion automatically manages depth-first exploration compared to manual stack management in iterative DFS, so don't worry too much, just memorize. \n\nBFS:\n\nMark node as visited: When enqueued (immediately after adding to the queue).\n\nWhy?: BFS processes nodes level by level, so marking when enqueuing prevents revisiting and ensures the shortest path is maintained.\n\nNo additional visited check: Since nodes are marked visited when enqueued, they won’t be added to the queue again, making an additional check after dequeuing unnecessary.\n\nKey Points:\n\nDFS explores deeply; multiple paths might push the same node, so check before pushing.\n\nBFS explores level by level; mark when enqueuing to ensure each node is processed once, in the correct order.\n\nEfficiency: BFS marking on enqueue is optimal for reducing redundant checks.\n\nDFS\n\nDFS magic spell: 1]push to stack, 2] pop top , 3] retrieve unvisited neighbours of top, push them to stack 4] repeat 1,2,3 while stack not empty. Now form a rap !\n\nRecursive DFS (adjacency list)\n\ndef dfs(node: int, visited: set, graph: dict):\n    # Mark the current node as visited\n    visited.add(node)\n    \n    # Process the current node (e.g., print, collect data, etc.)\n    print(f\"Visiting node {node}\")\n    \n    # Recursively visit all unvisited neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            dfs(neighbor, visited, graph)\n\nRecursive DFS (grid)\n\ndef dfs(x: int, y: int, visited: set, grid: list):\n    # Mark the current cell as visited\n    visited.add((x, y))\n    \n    # Process the current cell (e.g., print, collect data, etc.)\n    print(f\"Visiting cell ({x}, {y})\")\n    \n    # Define the directions for neighbors: up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Get the grid dimensions\n    rows, cols = len(grid), len(grid[0])\n    \n    # Recursively visit all unvisited neighbors within bounds\n    for dx, dy in directions:\n        nx, ny = x + dx, y + dy\n        if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited: #Here you can add additional conditions, like edge exists only if it is a 1 etc.\n            dfs(nx, ny, visited, grid)\n\nIterative DFS (adjacency list)\n\nWhen you push a node onto the stack, you're only checking if it's not visited yet at that moment. However, the same node might get added to the stack multiple times through different paths before it is actually processed. Thats the reason why we check visited both at the beginning and before adding neighbor. Think 1 - 2 - 3 - 4 loop. \n\nvisited = set()  # To track visited nodes\nstack = [start]  # Initialize the stack with the starting node\n\nwhile stack:\n    node = stack.pop()  # Pop the last node added (LIFO order)\n\n    if node not in visited:\n        # Mark the node as visited\n        visited.add(node)\n        print(f\"Visiting node {node}\")\n\n        # Push all unvisited neighbors onto the stack\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                stack.append(neighbor)\n\nIterative DFS (grid)\n\nvisited = set()  # To track visited cells\nstack = [(start_x, start_y)]  # Initialize the stack with the starting cell\n\n# Define the directions for neighbors: up, down, left, right\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n# Get the grid dimensions\nrows, cols = len(grid), len(grid[0])\n\nwhile stack:\n    x, y = stack.pop()  # Pop the last cell added\n\n    if (x, y) not in visited:\n        # Mark the current cell as visited\n        visited.add((x, y))\n        print(f\"Visiting cell ({x}, {y})\")\n\n        # Push all unvisited neighbors onto the stack (within bounds)\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:\n                stack.append((nx, ny))\n\nRecursive DFS to keep track of Path (adjacency list)\n\nThis works both in cyclic and acyclic graphs, as in the backtracking step, we are removing the node from the visited set once we finish exploring its neighbors. This prevents the algorithm from getting stuck in an infinite loop caused by cycles while still allowing revisits to nodes in different paths.\n\ndef dfs(node, target, graph, visited, path, all_paths):\n    visited.add(node)\n    path.append(node)\n    \n    if node == target:\n        # If we've reached the target, store the current path\n        all_paths.append(list(path))\n    else:\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                dfs(neighbor, target, graph, visited, path, all_paths)\n    \n    path.pop()  # Backtrack\n    visited.remove(node)\n\nRecursive DFS to keep track of Path (grid)\n\ndef dfs(x, y, target_x, target_y, grid, visited, path, all_paths):\n    # Add current cell to the path and mark it as visited\n    path.append((x, y))\n    visited.add((x, y))\n    \n    # If we reach the target cell, store the current path\n    if (x, y) == (target_x, target_y):\n        all_paths.append(list(path))  # Store a copy of the path\n    else:\n        # Explore the 4 possible directions: up, down, left, right\n        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n        rows, cols = len(grid), len(grid[0])\n\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            # Check if the next cell is within bounds and not visited\n            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited and grid[nx][ny] == 1:\n                dfs(nx, ny, target_x, target_y, grid, visited, path, all_paths)\n    \n    # Backtrack: remove the current cell from the path and unmark it as visited\n    path.pop()\n    visited.remove((x, y))\n\nRecursive DFS for topological sort (Directed Graph)\n\nKey point is, Once all neighbors of the current node have been processed, the current node is added to the stack.\n\nAfter performing DFS on all unvisited nodes, the stack will contain the nodes in reverse topological order (because nodes are pushed to the stack after their dependencies have been processed).\n\nResult: The topological order is obtained by reversing the stack.\n\nSome important points: This code will only work if there is no cycle, i.e , incase of a DAG. If you want it to work even when cycles are there, and return empty array if cycles are there, you need to add cycle detection logic. \n\ndef dfs_topological(node, graph, visited, stack):\n    visited.add(node)  # Mark the current node as visited\n\n    # Recursively visit all unvisited neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            dfs_topological(neighbor, graph, visited, stack)\n    \n    # After all neighbors are processed, add the current node to the stack\n    stack.append(node)\n\nvisited = set()  # Set to keep track of visited nodes\nstack = []  # Stack to store the topological order\n\n# Perform DFS from every node to ensure all nodes are visited\nfor node in range(n):\n    if node not in visited:\n        dfs_topological(node, graph, visited, stack)\n\n# The topological order is the reverse of the DFS post-order traversal\nreturn stack[::-1] \n\nRecursive DFS for Cycle Detection (Directed Graph)\n\nCycle detection is based on the Key point: In the current path, if there is back edge, i.e, node connecting to any previous nodes only in the current path, there is a cycle. \n\nYou cannot use visited to keep track of cycles, i.e claim that if we revisit the node there is a cycle, as a node maybe visited multiple times in DFS. \n\nYou also can't check something like if node in recursion_stack at the very beginning, because we will never visit the same node again due to us keeping track of visited. So that statement would never be True. So we always have to keep the main logic as detecting back edge. \n\ndef dfs_cycle(node, graph, visited, recursion_stack):\n    visited.add(node)  # Mark the node as visited\n    recursion_stack.add(node)  # Add the node to the current recursion stack\n\n    # Explore the neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            if dfs_cycle(neighbor, graph, visited, recursion_stack):\n                return True  # Cycle detected (If you don't do this, True won't be propogated)\n        elif neighbor in recursion_stack:\n            return True  # Cycle detected (back edge found)\n\n    # Backtrack: remove the node from the recursion stack\n    recursion_stack.remove(node)\n    return False\n\nBFS\n\nBFS (adjacency list)\n\nvisited = set()  # To track visited nodes\nqueue = deque([start])  # Initialize the queue with the starting node\nvisited.add(start)  # Mark the start node as visited when enqueuing\n\nwhile queue:\n    node = queue.popleft()  # Dequeue the first node in the queue\n    print(f\"Visiting node {node}\")  # Process the node (e.g., print or collect data)\n\n    # Enqueue all unvisited neighbors and mark them as visited when enqueuing\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            queue.append(neighbor)\n            visited.add(neighbor)  # Mark as visited when enqueuing\n\nBFS (grid)\n\nvisited = set()  # To track visited cells\nqueue = deque([(start_x, start_y)])  # Initialize the queue with the starting cell\nvisited.add((start_x, start_y))  # Mark the start cell as visited when enqueuing\n\n# Define the directions for neighbors: up, down, left, right\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\nrows, cols = len(grid), len(grid[0])\n\nwhile queue:\n    x, y = queue.popleft()  # Dequeue the first cell\n    print(f\"Visiting cell ({x}, {y})\")  # Process the current cell\n\n    # Enqueue all unvisited valid neighbors\n    for dx, dy in directions:\n        nx, ny = x + dx, y + dy\n        if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:\n            queue.append((nx, ny))\n            visited.add((nx, ny))  # Mark as visited when enqueuing\n\nMultisource BFS\n\nThe multi-source BFS pattern is useful when you need to start BFS from multiple starting points simultaneously. This pattern ensures that all sources are explored in parallel, and it's commonly used in problems like finding the shortest distance from multiple sources to a destination.\n\nThe only change is from normal BFS code is that you add all the source nodes in the queue and call BFS\n\nMaintaining Level information in BFS\n\nSimple way is just to maintain (node, level) instead of just node. Each time you are enqueuing new nodes, increment the level by 1. This way, you have level information for all the nodes. In this method, level information will be lost at the end, as the queue will become empty. It can still be used if you only need the end result, but if you need information like no. of nodes in each level etc., It is better to use level processing approach. \n\nOther way is using array for levels, like so. Idea is to process nodes level by level, tracking the current level by processing all nodes at the same depth in one batch, and incrementing the level after processing each layer. Useful in tree problems (level order traversal) too.\n\nvisited = set([start])  # Track visited nodes, starting with the source node\nqueue = deque([start])  # Queue to store nodes to be processed\nlevel = 0  # Start from level 0 (the level of the start node)\n\nwhile queue:\n    # Get the number of nodes at the current level\n    level_size = len(queue)\n    \n    # Process all nodes at the current level\n    for _ in range(level_size):\n        node = queue.popleft()  # Pop a node from the queue\n        print(f\"Node: {node}, Level: {level}\")\n        \n        # Add unvisited neighbors to the queue\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n    \n    # After processing all nodes at the current level, increment the level\n    level += 1\n\nBFS for topological sort (Cycle detection built in) (Kahn's algorithm)\n\ngraph = defaultdict(list)  # Adjacency list representation of the graph\nin_degree = [0] * n  # In-degree of each node\n\n# Build the graph and calculate in-degrees\nfor start, end in edges:\n    graph[start].append(end)\n    in_degree[end] += 1\n\n# Initialize the queue with all nodes that have in-degree of 0\nqueue = deque([i for i in range(n) if in_degree[i] == 0])\ntopo_order = []\n\nwhile queue:\n    node = queue.popleft()  # Get the node with zero in-degree\n    topo_order.append(node)  # Add it to the topological order\n    \n    # Reduce in-degree of all its neighbors\n    for neighbor in graph[node]:\n        in_degree[neighbor] -= 1\n        # If a neighbor now has in-degree of 0, add it to the queue\n        if in_degree[neighbor] == 0:\n            queue.append(neighbor)\n\n# If all nodes are processed, return the topological order, otherwise return empty (cycle detected)\nif len(topo_order) == n:\n    return topo_order\nelse:\n    return []  # Cycle detected\n\nEulerian Path/Cycle\n\nEulerian Path: If there is exactly one vertex with out-degree greater by 1 and one with in-degree greater by 1.\n\nEulerian Cycle: If all vertices have equal in-degree and out-degree\n\nBasically Eulerian Path/Cycle means we cover all edges of a graph exactly once \n\nThe algorithm is simple, we recursively keep removing edges one by one, so no need of visited set as we can revisit a node multiple times, but cannot revisit an edge. More elegant implementations also exist, but this should suffice for this rare problem. \n\ndef dfs_eularian(node, graph, stack):\n    while graph[node]:\n        next_node = graph[node].pop(0) # Only if lexcial order pop(0), else its fine to pop any neighbor\n        dfs_eularian(next_node)\n    stack.append(node)\n\n# Start DFS from the determined starting airport\ndfs_eularian(start) # For determining start node, follow the instructions in the notes above.\nreturn stack[::-1]  # Reverse the itinerary to get the correct order\n\nDisjoint Set Union / Union Find\n\nPurpose: DSU is used to manage and merge disjoint sets, mainly in graph problems for tracking connected components and detecting cycles.\n\nKey Operations:\n\nFind with Path Compression: Reduces the time complexity by flattening the tree, so future operations are faster.\n\nUnion by Rank/Size: Keeps the tree balanced by attaching the smaller tree under the root of the larger one.\n\nTime Complexity: Both find and union have nearly constant time complexity, due to path compression and union by rank\n\nCommon Use Cases:\n\nCycle Detection in undirected graphs.\n\nKruskal’s MST Algorithm to avoid cycles when adding edges.\n\nConnected Components to check if two nodes are in the same component.\n\nInitialization: Use two arrays—parent (each node points to itself initially) and rank (initially 0 for all nodes).\n\nclass DSU:\n    def __init__(self, n):\n        # Initialize parent and rank arrays\n        self.parent = [i for i in range(n)]\n        self.rank = [0] * n\n\n    def find(self, x):\n        '''\n        # Intialize parent and rank as dicts {} if no. of nodes is not known, and just add these lines of code\n        # Initialize parent and rank if node is encountered for the first time\n        if x not in self.parent:\n            self.parent[x] = x\n            self.rank[x] = 0\n        '''\n        # Find the root of the set containing x with path compression\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n\n    def union(self, x, y):\n        # Union by rank\n        root_x = self.find(x)\n        root_y = self.find(y)\n\n        if root_x != root_y:\n            # Attach smaller rank tree under root of the higher rank tree\n            if self.rank[root_x] > self.rank[root_y]:\n                self.parent[root_y] = root_x\n            elif self.rank[root_x] < self.rank[root_y]:\n                self.parent[root_x] = root_y\n            else:\n                self.parent[root_y] = root_x\n                self.rank[root_x] += 1\n\n    def connected(self, x, y):\n        # Check if two elements are in the same set\n        return self.find(x) == self.find(y)\n\nMinimum Spanning Trees (MST)\n\nAn MST connects all nodes in an undirected, weighted graph with the minimum possible total edge weight, ensuring there are no cycles and the graph remains fully connected.\n\nMST Kruskal's\n\nApproach: Edge-based, Greedy\n\nProcess:\n\nSort all edges in non-decreasing order by weight.\n\nInitialize an empty MST and start adding edges from the sorted list.\n\nFor each edge, check if it forms a cycle using DSU. If not, add it to the MST.\n\nRepeat until the MST has V-1 exactly edges (where V is the number of vertices).\n\nBest for: Sparse graphs where sorting edges is manageable.\n\ndef kruskal_mst_fixed(edges):\n    # Initialize DSU and collect unique nodes to determine number of nodes\n    dsu = DynamicDSU()\n    unique_nodes = set(u for u, v, _ in edges).union(set(v for u, v, _ in edges))\n    num_nodes = len(unique_nodes)\n\n    # Sort edges by weight (ascending order)\n    edges.sort(key=lambda x: x[2])\n\n    mst = []  # To store edges in MST\n    total_cost = 0\n\n    # Iterate through sorted edges\n    for u, v, weight in edges:\n        # Only add edge if it doesn't form a cycle\n        if not dsu.connected(u, v):\n            dsu.union(u, v)  # Union the two vertices\n            mst.append((u, v, weight))  # Add edge to MST\n            total_cost += weight  # Add edge weight to total cost\n\n            # Stop if MST has enough edges (n - 1 edges for n nodes)\n            if len(mst) == num_nodes - 1:\n                break\n\n    return mst, total_cost\n\nMST Prim's\n\nApproach: Vertex-based, Greedy\n\nProcess:\n\nStart from any initial node.\n\nUse a min-heap (priority queue) to track edges that extend from the MST.\n\nRepeatedly pop the minimum edge from the heap:\n\nIf it connects to an unvisited node, add it to the MST and add its neighbors to the heap.\n\nStop when the MST includes all nodes.\n\nBest for: Dense graphs with adjacency lists/matrices.\n\n# Redefining Prim's algorithm for MST with adjacency list input\ndef prim_mst(graph, start):\n    # Initialize structures\n    mst = []  # To store the edges in the MST\n    total_cost = 0  # To accumulate the total weight of the MST\n    visited = set()  # Track nodes already included in the MST\n    min_heap = []  # Priority queue (min-heap) for edges\n\n    # Function to add edges to the priority queue\n    def add_edges(node):\n        visited.add(node)\n        for neighbor, weight in graph[node]:\n            if neighbor not in visited:\n                heapq.heappush(min_heap, (weight, node, neighbor))\n\n    # Start from the initial node\n    add_edges(start)\n\n    # Process until MST includes all nodes or min-heap is empty\n    while min_heap and len(visited) < len(graph):\n        weight, u, v = heapq.heappop(min_heap)\n        if v not in visited:  # Only add edge if it connects to an unvisited node\n            mst.append((u, v, weight))\n            total_cost += weight\n            add_edges(v)  # Add edges from the newly added node\n\n    return mst, total_cost\n\nShortest Path Dijkstra\n\nApproach: Single-source shortest path for non-negative weights\n\nProcess:\n\nInitialize distances from the start node to all other nodes as infinity (except for the start, set to 0).\n\nUse a min-heap to manage nodes by their current shortest distance.\n\nPop the node with the smallest distance:\n\nFor each neighbor, calculate the potential new distance.\n\nIf this distance is shorter than the known distance, update it and push the neighbor with the updated distance.\n\nContinue until all reachable nodes have the shortest path from the start.\n\nBest for: Shortest paths in non-negative weighted graphs.\n\ndef dijkstra(graph, start):\n    # Initialize distance dictionary with infinity for all nodes except the start\n    distances = {node: float('inf') for node in graph}\n    distances[start] = 0\n\n    # Priority queue (min-heap) initialized with the starting node\n    min_heap = [(0, start)]  # (distance, node)\n\n    while min_heap:\n        # Pop the node with the smallest distance\n        current_distance, u = heapq.heappop(min_heap)\n\n        # Process only if the current distance is the smallest known distance for u\n        if current_distance > distances[u]:\n            continue\n\n        # Check each neighbor of the current node\n        for v, weight in graph[u]:\n            distance = current_distance + weight  # Calculate potential new distance to neighbor\n\n            # Only consider this path if it's shorter than the known distance\n            if distance < distances[v]:\n                distances[v] = distance  # Update to the shorter distance\n                heapq.heappush(min_heap, (distance, v))  # Push updated distance into the heap\n\n    return distances\n\nTips and Tricks to Solve graph problems\n\nTo detect length of cycle or elements in cycle, you can keep track of entry times in the recursive_stack. This can also help you in finding the exact cycle. \n\nFor undirected graphs, cycles are found using DSU or DFS with back edges. For directed graphs, cycles are detected through DFS with recursion stack tracking.\n\nDynamic Programming\n\nGeneral tip - In bottom up DP, if 2D DP, draw the matrix and visualize the dependencies, becomes easier.\n\nSometime memoization is more intuitive, sometime DP is more intuitive. DP you can usually perform space optimization. \n\n0/1 Knapsack Pattern - \n\ndef knapsack(values, weights, capacity):\n    n = len(values)\n    dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n    \n    # Fill the DP table\n    for i in range(1, n + 1):  # For each item\n        for w in range(1, capacity + 1):  # For each capacity\n            if weights[i - 1] <= w:\n                dp[i][w] = max(dp[i - 1][w], values[i - 1] + dp[i - 1][w - weights[i - 1]]) #i-1 is the actual item\n            else:\n                dp[i][w] = dp[i - 1][w]\n    \n    return dp[n][capacity]\n\nUnbounded Knapsack Patten - 322, 343, 279\n\ndef unbounded_knapsack(values, weights, capacity):\n    n = len(values)\n    dp = [0] * (capacity + 1)\n    \n    # Fill the DP array\n    for i in range(n):  # For each item\n        for w in range(weights[i], capacity + 1):  # For each capacity\n            dp[w] = max(dp[w], values[i] + dp[w - weights[i]])\n    \n    return dp[capacity]\n\nCoin Change II - Since ordering doesnt matter, it is a 2 state problem instead of 1 state. little tricky to catch. You use inclusion/exclusion decision tree, memoization solution is simple to implement. \n\nIf ordering does matter, then it is a simple 1 state solution like Coin Change I\n\nFibonacci\n\nThe Fibonacci pattern shows up in many dynamic programming problems where each state depends on a fixed number of previous states.\n\ndef fibonacci_dp(n):\n    if n <= 1:\n        return n\n    dp = [0] * (n + 1)\n    dp[1] = 1\n    for i in range(2, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\nLongest Palindromic Substring\n\nIf you know that a substring s[l+1:r-1] is a palindrome, then s[l:r] is also a palindrome if s[l] == s[r].\n\nIn problems involving palindromic substrings or subsequences, the goal is often to:\n\nIdentify the longest palindromic substring (continuous sequence).\n\nCount the number of palindromic substrings.\n\nFind the longest palindromic subsequence (which doesn’t need to be contiguous).\n\nImportant: Diagonal Filling of the matrix\n\ndef longestPalindrome(self, s: str) -> str:\nn= len(s)\ndp = [[False]*n for _ in range(n)]\n\nfor i in range(n):\n    dp[i][i] = True\nans = [0,0]\n\nfor i in range(n - 1):\n    if s[i] == s[i + 1]:\n        dp[i][i + 1] = True\n        ans = [i, i + 1]\n\nfor i in range(n-1, -1, -1):\n    for j in range(n-1, -1, -1):\n        if j<=i+1:\n            continue\n        if s[i]==s[j] and dp[i+1][j-1]:\n            dp[i][j] = True\n            if j-i>ans[1]-ans[0]:\n                ans = [i,j]\nreturn s[ans[0]:ans[1]+1]\n\nMaximum Sum/Product Subarrays\n\ndef max_subarray_sum(nums):\n    max_sum = nums[0]\n    current_sum = nums[0]\n    \n    for i in range(1, len(nums)):\n        current_sum = max(nums[i], current_sum + nums[i])\n        max_sum = max(max_sum, current_sum)\n    \n    return max_sum\n\ndef maxProduct(nums):\n        prev_max, prev_min = nums[0], nums[0]\n        ans=prev_max\n        for i in range(1,len(nums)):\n            curr_max = max(nums[i], nums[i]*prev_max, nums[i]*prev_min)\n            curr_min = min(nums[i], nums[i]*prev_max, nums[i]*prev_min)\n            prev_max, prev_min = curr_max, curr_min\n            ans = max(ans, prev_max)\n        return ans\n\nWord Break\n\n#O(n^2), dp[i] represents if word till i can be broken into parts. Important problem as you need to check all previous indices. \n\nLongest Increasing Subsequence\n\nThe Longest Increasing Subsequence (LIS) Pattern is a common dynamic programming pattern used to find subsequences within a sequence that meet certain increasing criteria. This pattern typically involves identifying or counting subsequences (not necessarily contiguous) that satisfy conditions related to increasing order, longest length, or specific values.\n\nDefine the DP Array: Use an array dp where dp[i] represents the length of the longest increasing subsequence ending at index i.\n\nTransition: For each element i, check all previous elements j < i. If nums[j] < nums[i], update dp[i] = max(dp[i], dp[j] + 1) to extend the subsequence ending at j.\n\nImportant: O(nlogn) Use tails to store the smallest ending of increasing subsequences. For each num, use binary search to find its position in tails – replace if within bounds, or append if beyond (is the last element)\n\ndef length_of_lis(nums):\n    \n    dp = [1] * len(nums)  # Each element is at least an increasing subsequence of length 1\n    \n    for i in range(1, len(nums)):\n        for j in range(i):\n            if nums[j] < nums[i]:\n                dp[i] = max(dp[i], dp[j] + 1)\n    \n    return max(dp)\n\nCounting Paths/Combinations Pattern\n\nGoal: Given a target, find distinct ways to reach it based on given moves/rules.\n\nDP Array/Table: Use dp[i] or dp[i][j] to store the count of ways to reach each target or cell.\n\nCommon Formula: For each i, update dp[i] by summing counts from preceding states based on allowed moves.\n\nKey Examples\n\nClimbing Stairs: dp[i] = dp[i-1] + dp[i-2]\n\nCoin Change (Combinations): dp[i] += dp[i - coin] for each coin\n\nGrid Unique Paths: dp[i][j] = dp[i-1][j] + dp[i][j-1]\n\nLongest Common Subsequence (LCS) Pattern\n\nGoal: Compare two sequences and find:\n\nLength of Longest Common Subsequence (LCS).\n\nMinimum edits to transform one sequence into another (Edit Distance).\n\nLongest contiguous substring (Longest Common Substring).\n\nKey Idea: Use a 2D DP table dp[i][j] where:\n\ndp[i][j] represents the result for substrings s1[:i] and s2[:j].\n\nBase Cases: If i == 0 or j == 0, the result is 0 (empty string).\n\ndef longest_common_subsequence(text1, text2):\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i - 1] == text2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n    return dp[m][n]\n\nVariants\n\nEdit Distance: Modify dp[i][j] transition to consider insert, delete, substitute. Also modify to correct base case. \n\ndp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+(1 if s1[i-1]!=s2[j-1] else 0))\n\nLongest Common Substring: If characters match, add +1 to previous diagonal:\n\nif text1[i - 1] == text2[j - 1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = 0\n\nState based DP Pattern\n\nGoal: Optimize decisions across states influenced by actions (e.g., buying/selling, resting/working).\n\nKey Idea: Define states to track situations (e.g., holding stock, not holding, cooldown) and transition equations between states.\n\nSteps:\n\nIdentify states based on possible actions.\n\nDraw a state diagram to visualize transitions.\n\nWrite recurrence relations for each state based on dependencies.\n\nBase Cases: Define initial conditions (e.g., starting with no stock or zero profit).\n\ndef state_based_dp_problem(prices):\n    # Base cases (initial states)\n    hold = -prices[0]   # Max profit when holding stock on day 0\n    not_hold = 0        # Max profit when not holding stock on day 0\n    cooldown = 0        # Max profit in cooldown on day 0\n\n    for i in range(1, len(prices)):\n        prev_hold = hold\n        # State transitions\n        hold = max(hold, not_hold - prices[i])        # Continue holding or buy today\n        not_hold = max(not_hold, cooldown)           # Continue not holding or end cooldown\n        cooldown = prev_hold + prices[i]             # Sell today and enter cooldown\n\n    # Final result: max profit can be in not_hold or cooldown\n    return max(not_hold, cooldown)\n\nTodo\n\nNegative marking \n\nPrefix Sum + Hashmap pattern (also modulo if involved)\n\nSliding window + Hashmap (also using matched variable to avoid hashmap)\n\nMonotonic Stack\n\nQuickSelect Algorithm \n\nStoring index as key in hashmap (Problem #3 leetcode)\n\nSliding window - complement of sliding window pattern 2516. Take K of Each Character From Left and Right\n\nArrays + Hashmaps\n\nQuickSelect \n\nUse Cases:\n\nKth largest/smallest element or Top K elements in O(n) average time.\n\nAvoids full sorting for subset problems (better than nlogn).\n\nKey Insight:\n\nPartition around a pivot to partially sort: left satisfies the comparator, pivot lands in its correct position.\n\nHow to Use:\n\nSet k = k - 1 for 0-based indexing.\n\nUse x >= y for descending order (Kth largest, Top K).\n\nUse x <= y for ascending order (Kth smallest, Bottom K).\n\ndef quickselect(arr, left, right, k, comparator):\n    def partition(arr, left, right):\n        pivot = arr[right]  # Choose the last element as pivot\n        p = left  # Pointer for elements satisfying comparator\n        for i in range(left, right):\n            if comparator(arr[i], arr[right]):  # Compare with pivot\n                arr[i], arr[p] = arr[p], arr[i]\n                p += 1\n        arr[p], arr[right] = arr[right], arr[p]  # Place pivot in position\n        return p\n\n    if left <= right:\n        pivot_index = partition(arr, left, right)\n        if pivot_index == k:  # Found the Kth element\n            return arr[pivot_index]\n        elif pivot_index < k:  # Look for Kth in the right part\n            return quickselect(arr, pivot_index + 1, right, k, comparator)\n        else:  # Look for Kth in the left part\n            return quickselect(arr, left, pivot_index - 1, k, comparator)\n\n# Comparator for kth largest and top k elements \ncomparator = lambda x, y: x >= y\n\n# Comparator for kth smallest and bottom k elements \ncomparator = lambda x, y: x <= y\n\nPrefix Sum (Prefix Sum, Prefix Sum+Binary Search, Prefix Sum+Hashmaps)\n\nKey Idea: Combine prefix sums with a hashmap to store cumulative sums and solve subarray problems dynamically.\n\nprefix[i] - prefix[j] = k => prefix[j] = prefix[i] - k\n\nUse Cases:\n\nCount subarrays with specific conditions (e.g., sum equals k).\n\nModular conditions (e.g., divisible by k).\n\nImportant points: Particularly useful when sliding window approaches fail (because of presence of negative numbers), If only +ve numbers are present, prefix_sum array is sorted, can think if binary search is needed.\n\ndef prefix_sum_with_hashmap(arr, k):\n    prefix = 0\n    count = 0\n    hashmap = {0: 1}  # Initialize with prefix 0 to handle exact matches\n    \n    for num in arr:\n        prefix += num\n        # Check if prefix - k exists in the hashmap\n        if (prefix - k) in hashmap:\n            count += hashmap[prefix - k]\n        # Update the hashmap with the current prefix\n        hashmap[prefix] = hashmap.get(prefix, 0) + 1\n    \n    return count\n\nHashsets (Building consecutive sequences pattern)\n\nKey Idea:\n\nUse a HashSet for:\n\nQuick lookups for presence/absence.\n\nEnsuring uniqueness of elements.\n\nProblems involving element relationships (e.g., consecutive sequences).\n\ndef longest_consecutive(nums):\n  \n    num_set = set(nums)\n    longest = 0\n    \n    for num in num_set:\n        # Check if it's the start of a sequence\n        if num - 1 not in num_set:\n            length = 0\n            current = num\n            while current in num_set:\n                length += 1\n                current += 1\n            longest = max(longest, length)\n    \n    return longest\n\nSorting\n\n# 1. Sort in Ascending Order\narr.sort()  # [1, 3, 5, 8]\n\n# 2. Sort in Descending Order\narr.sort(reverse=True)  # [8, 5, 3, 1]\n\n# 3. Sort by the Second Element in a Tuple\narr.sort(key=lambda x: x[1])  # [(3, 1), (1, 2), (2, 3)]\n\n# 4. Sort by Length of Strings\narr.sort(key=lambda x: len(x))  # ['kiwi', 'apple', 'banana']\n\n# 5. Sort by Multiple Keys (Primary Ascending, Secondary Descending)\narr.sort(key=lambda x: (x[0], -x[1]))  # [(1, 3), (1, 2), (2, 3), (2, 2)]\n\n# 6. Sort by the second element in a tuple using sorted()\nsorted_arr = sorted(arr, key=lambda x: x[1])  # [(3, 1), (1, 2), (2, 3)]\n\nNegative Marking\n\nThe problem involves integers bounded by the size of the array (e.g., values from 1 to n). \n\nYou’re asked to find duplicates, missing elements, or cycles in O(n) time and O(1) space. \n\nThe input array can be modified in-place.\n\ndef find_duplicates(nums): #nums has only values 1 to n\n    res = []\n    for num in nums:\n        index = abs(num) - 1  # Map value to index\n        if nums[index] < 0:\n            res.append(abs(num))  # Already marked negative -> duplicate\n        else:\n            nums[index] = -nums[index]  # Mark as visited\n    return res\n\nMajority Element (Boyer-Moore Voting Algorithm + Hashmap Alternative)\n\nCandidate votes for itself, all other candidates vote against it\n\ndef majority_element(nums):\n   \n    # Step 1: Find the candidate\n    candidate, count = None, 0\n    for num in nums:\n        if count == 0:\n            candidate = num\n        count += 1 if num == candidate else -1\n\n2 pointers\n\nOpposite Direction Two Pointers\n\nSorting may be useful if not explicitly prohibited\n\nIn the Opposite Direction Two Pointers pattern, two pointers are initialized at opposite ends of a list or array, and they are moved toward each other based on conditions to solve a problem. This approach is widely applicable to problems involving sorted arrays, searching for optimal solutions, or evaluating complex conditions involving multiple indices.\n\nUse l < r: For problems comparing pairs of elements where overlapping isn't meaningful (most problems, 2 sum sorted etc). Use l <= r: For problems where overlapping or single element checks are necessary (palindrome check). \n\ndef opposite_direction_two_pointers(arr, condition):\n    left, right = 0, len(arr) - 1\n    \n    while left < right:\n        # Perform actions based on condition\n        if condition(arr[left], arr[right]):\n            # Example: process a valid pair\n            process(arr[left], arr[right])\n        \n        # Update pointers based on the problem logic\n        if move_left_condition:  # Replace with actual condition\n            left += 1\n        elif move_right_condition:  # Replace with actual condition\n            right -= 1\n        else:\n            # Break if no further action is possible\n            break\n\nGeneral Problem Categories\n\nSimple Conditions:\n\nProblems with straightforward conditions for moving pointers (e.g., sums, comparisons, or matching characters).\n\nExamples:\n\nTwo-Sum in a sorted array.\n\nChecking if a string is a palindrome.\n\nComplex Conditions:\n\nProblems where the condition to move pointers involves more elaborate calculations or logic.\n\nExamples:\n\nMaximizing or minimizing values (e.g., Container with Most Water).\n\nAggregating values across the pointers (e.g., Trapping Rain Water).\n\nAdvanced Applications:\n\nProblems that incorporate sorting or nested loops (e.g., counting triplets or evaluating multiple conditions)\n\nExamples:\n\nValid triangle number, 3-sum\n\nNote: Was not able to solve 3-sum (including all duplicates) using this method\n\ndef opposite_direction_with_sorting(arr):\n    # Step 1: Sort the array if needed\n    arr.sort()\n\n    # Step 2: Iterate through the array with one fixed pointer\n    for i in range(len(arr)):\n        left, right = i + 1, len(arr) - 1 #Can be adjusted, you can start at end of array too, like Valid triangle\n        \n        # Step 3: Use two pointers to evaluate the condition\n        while left < right:\n            if condition(arr[i], arr[left], arr[right]): \n                process(arr[i], arr[left], arr[right])\n                # Adjust pointers based on requirements\n                left += 1\n                right -= 1\n            elif adjust_left_condition:  # Example condition to move left\n                left += 1\n            else:  # Adjust right\n                right -= 1\n\nSame Direction Pointers (Not Sliding Window)\n\nThis pattern involves two pointers (left and right) that move in the same direction. The right pointer moves first until a condition is met or unmet. Once the condition changes, the left pointer is adjusted to the position of the right pointer. This is not a sliding window since the left pointer does not increment gradually but instead jumps to match the right pointer.\n\nKey Insights\n\nRight Pointer Moves First:\n\nStart with left and right at the same position.\n\nIncrement right while evaluating a condition.\n\nAdjust Left Pointer:\n\nWhen the condition is violated or met, bring the left pointer to match the right pointer.\n\nNon-Overlapping Subarrays:\n\nThis pattern ensures that the ranges between left and right pointers are disjoint or non-overlapping.\n\ndef same_direction_pointers(arr):\n    left = 0\n    right = 0\n    while right < len(arr):\n        # Expand the right pointer\n        while right < len(arr) and condition(arr[right]):\n            #Can have complex logic here instead of having condition above\n            right += 1\n        \n        # Process the range [left, right)\n        process(arr[left:right])\n        \n        # Increment right pointer by 1 to start new window\n        right+=1\n        # Move the left pointer to match the right\n        left = right\n\nCommon Applications\n\nSplitting Strings or Arrays:\n\nBreaking a sequence into segments based on a condition.\n\nProcessing Subarrays:\n\nAnalyze non-overlapping subarrays meeting specific criteria.\n\nCounting or Extracting Ranges:\n\nCount segments, extract substrings, or find subarrays.\n\nSkipping Invalid Values:\n\nHandle sequences with gaps or delimiters by skipping invalid parts.\n\nThree pointers\n\nBasically partition array into three groups by some conditions.\n\ndef sort_colors(nums):\n    p1, p2, p3 = 0, 0, len(nums) - 1\n\n    while p2 <= p3:\n        if nums[p2] == 0:  # Move 0s to the left\n            nums[p1], nums[p2] = nums[p2], nums[p1]\n            p1 += 1\n            p2 += 1\n        elif nums[p2] == 1:  # Keep 1s in the middle\n            p2 += 1\n        else:  # Move 2s to the right\n            nums[p2], nums[p3] = nums[p3], nums[p2]\n            p3 -= 1\n\nPattern: Two Pointers on Two Different Arrays/Strings\n\nThis pattern involves using two pointers, each operating on a separate array or string. The pointers traverse independently or interact based on specific conditions to solve a problem efficiently.\n\ndef two_pointers_on_two_arrays(arr1, arr2):\n    # Initialize two pointers\n    i, j = 0, 0\n    result = []\n\n    while i < len(arr1) and j < len(arr2):\n        if condition(arr1[i], arr2[j]):\n            process(arr1[i], arr2[j], result)\n            i += 1\n            j += 1\n        elif adjust_pointer_1_condition:\n            i += 1\n        else:\n            j += 1\n\n    # Process remaining elements if required\n    while i < len(arr1):\n        process(arr1[i], None, result)\n        i += 1\n    while j < len(arr2):\n        process(None, arr2[j], result)\n        j += 1\n\n    return result\n\nPattern: Fast and Slow Pointers on Arrays (Tortoise and Hare) \n\nJust writing it down here, not very important, revisit if you have time. 2 problems - LeetCode 457: Circular Array Loop, LeetCode 202: Happy Number\n\nSliding Window\n\nFixed Size\n\ndef fixed_size_sliding_window(arr, k):\n    n = len(arr)\n    window_sum = 0\n    result = []\n\n    # Initialize the first window\n    for i in range(k):\n        window_sum += arr[i]\n    \n    # Append the result of the first window\n    result.append(window_sum)\n\n    # Slide the window across the array\n    for i in range(k, n):\n        window_sum += arr[i] - arr[i - k]  # Add the next element, remove the first element of the previous window\n        result.append(window_sum)\n    \n    return result\n\nDynamic Size\n\nl = 0\nfor r in range(len(nums)):\n    # Expand window by adding nums[r]\n    update_window(nums[r])\n\n    # Shrink window if condition is violated\n    while condition_not_met():\n        # Update result if required inside the loop (e.g., for min problems)\n        update_window_on_shrink(nums[l])\n        l += 1\n\n    # Update result if required outside the loop (e.g., for max problems)\n    update_result(l, r)\n\nDynamic Sliding Window Notes\n\nGeneral Rules:\n\nUse two pointers (l, r): expand with r, shrink with l.\n\nUpdate result inside while if intermediate windows matter (e.g., min problems).\n\nUpdate result after while if only the final window matters (e.g., max problems).\n\nSliding Window + HashMap/Set:\n\nUse a hashmap/set to track element frequencies or uniqueness.\n\nShrink when the hashmap/set exceeds constraints (e.g., distinct elements > k).\n\nIndex Trick (Last Occurrence):\n\nUse a hashmap to store the last occurrence of an element.\n\nUpdate l to max(l, last_occurrence + 1) to skip invalid windows.\n\nMatches Trick (Two HashMaps):\n\nUse two hashmaps and a matches variable to track when the window satisfies the target hashmap.\n\nIncrement matches when counts match; decrement on invalid shrink.\n\nSliding Window + Prefix Sum:\n\nUse prefix sums to compute subarray sums efficiently.\n\nTrack prefix sums in a hashmap for difference-based lookups. \n\nSliding Window + Deque:\n\nUse a deque to maintain a monotonic order of indices/values in the window.\n\nUseful for problems like finding max/min in a sliding window. (Covered in detail in queues section)\n\nComplement of Sliding Window\n\nThe answer lies outside the window (example select minimum/maximum something from left and right) problem 2516. Take K of Each Character From Left and Right\n\nQuirks and Edge Cases:\n\nFor substring problems, slicing (s[l:r+1]) is useful.\n\nSliding window can combine with binary search for length checks. \n\nTwo-pass sliding window works when expansion and shrinking need separate logic.\n\nStacks\n\nStack Simulation Pattern\n\n1. Direct Stack Usage \n\nUse Case: Maintain elements or operations in a stack to process them in LIFO order.\n\nKey Steps:\n\nPush elements onto the stack as needed.\n\nPop elements off the stack to resolve conditions or complete operations.\n\nExample problems - Valid Paranthesis, \n\nstack = []\nfor char in input_sequence:\n    if condition_to_push(char):\n        stack.append(char)\n    elif condition_to_pop(stack, char):\n        stack.pop()\nreturn result_based_on_stack(stack)\n\n2. Auxiliary Stack for Tracking\n\nUse Case: Use a secondary stack to track auxiliary information (e.g., min/max, wildcards).\n\nKey Steps:\n\nUse the main stack for primary operations.\n\nUse the auxiliary stack to maintain additional data in sync.\n\nSynchronize both stacks during push and pop operations.\n\nstack, aux_stack = [], []\nfor char in input_sequence:\n    if condition_to_push(char):\n        stack.append(char)\n        aux_stack.append(update_aux(char, aux_stack))\n    elif condition_to_pop(stack, char):\n        stack.pop()\n        aux_stack.pop()\nreturn result_based_on_aux(aux_stack)\n\nMonotonic Stack\n\nOverview:\n\nDefinition: A stack that maintains elements in a strictly increasing or decreasing order (monotonic).\n\nUse Case: Solve problems involving nearest greater/smaller elements, range computations, and areas/volumes efficiently.\n\nKey Ideas:\n\nPush elements while maintaining the order.\n\nPop elements when the order is violated, typically while processing conditions.\n\nTwo Types of Monotonic Stacks:\n\n1. Monotonically Increasing Stack\n\nMaintains elements in increasing order.\n\nUsage: \"Next Smaller Element\" or \"Nearest Smaller Element\" problems. (Largest Rectangle in Histogram)\n\ndef monotonically_increasing_stack(array):\n    stack = []\n    result = [-1] * len(array)  # Result to store required output (e.g., next smaller/greater)\n\n    for i, val in enumerate(array):\n        while stack and array[stack[-1]] > val:\n            index = stack.pop()\n            result[index] = val  # Process the popped element (e.g., update result)\n        stack.append(i)  # Push the current index onto the stack\n\n    return result\n\n2. Monotonically Decreasing Stack\n\nMaintains elements in decreasing order.\n\nUsage: \"Next Greater Element\" or \"Nearest Greater Element\" problems. (Trapping Rain Water)\n\ndef monotonically_decreasing_stack(array):\n    stack = []\n    result = [-1] * len(array)  # Result to store required output (e.g., next smaller/greater)\n\n    for i, val in enumerate(array):\n        while stack and array[stack[-1]] < val:\n            index = stack.pop()\n            result[index] = val  # Process the popped element (e.g., update result)\n        stack.append(i)  # Push the current index onto the stack\n\n    return result\n\nQueue\n\nMonotonic Queue (decreasing example, for sliding window maximum)\n\nKey Idea:\n\nRemove elements from the front of the queue if they are not part of the current window\n\nRemove elements from the back of the queue that are smaller than the current element to maintain the order.\n\nfrom collections import deque\n\ndef monotonic_decreasing_queue(nums, k):\n    queue = deque()\n    result = []\n\n    for i, num in enumerate(nums):\n        # Remove indices out of the current sliding window\n        if queue and queue[0] < i - k + 1:\n            queue.popleft()\n\n        # Remove elements smaller than the current element from the back\n        while queue and nums[queue[-1]] < num:\n            queue.pop()\n\n        queue.append(i)  # Add current index to the queue\n\n        # Add the maximum for the current window to the result\n        if i >= k - 1:\n            result.append(nums[queue[0]])\n\n    return result\n\nBinary Search\n\ndef binary_search_variant(arr, target):\n    left, right = 0, len(arr) - 1\n    result = -1  # Initialize result if needed\n    while left <= right:\n        mid = left + (right - left) // 2\n        if arr[mid] ? target:  # Replace '?' with the appropriate operator\n            result = mid  # Update result if needed\n            # Decide whether to move left or right based on the pattern\n            right = mid - 1 or left = mid + 1\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return result\n\nBinary Search on Answer: Concise Notes\n\n1. How to Identify\n\nOptimization Problem:\n\nFind the minimum or maximum value satisfying a condition (e.g., minimize largest, maximize smallest).\n\nMonotonic Search Space:\n\nIf a value satisfies the condition, all larger (or smaller) values also satisfy it.\n\nHelper Function:\n\nA function can_satisfy(value) exists to verify if a value meets the condition (usually O(n)).\n\nCommon Problems\n\nSplit Array Largest Sum (Minimize largest subarray sum).\n\nKoko Eating Bananas (Minimize eating speed).\n\nAggressive Cows (Maximize minimum distance).\n\nAllocate Minimum Pages (Minimize pages per student).\n\nKey Idea: Binary search efficiently narrows the range of possible answers; the helper function validates feasibility at each step.\n\ndef binary_search_on_answer(arr, condition_fn, low, high):\n    while low <= high:\n        mid = low + (high - low) // 2\n        if condition_fn(mid, arr):\n            high = mid - 1  # Try for a smaller/larger valid value\n        else:\n            low = mid + 1  # Discard invalid values\n    return low\n\nBinary Search on Unsorted Arrays: Concise Notes\n\n1. When to Apply\n\nInput is not fully sorted, but the problem has a monotonic property or a specific structure:\n\nRotated Sorted Arrays.\n\nPeak Element or Mountain Arrays.\n\nVirtual/Conceptual Search Spaces (e.g., infinite arrays).\n\nKey Requirement:\n\nThe search space or conditions allow the array to be divided into parts where binary search can narrow down the range.\n\n2. Key Problems\n\nSearch in Rotated Sorted Array: Narrow down the range using rotation logic.\n\nFind Peak Element: Use binary search to locate a peak.\n\nFind Minimum in Rotated Array: Locate the point of rotation.\n\n3. Template\n\nCore Idea: Identify the property to decide which half of the array to discard.\n\n4. Examples: \n\nProblem Find Peak Element: Find an element that is greater than its neighbors in an unsorted array.\n\ndef find_peak_element(nums):\n    left, right = 0, len(nums) - 1\n    while left < right:\n        mid = left + (right - left) // 2\n        if nums[mid] < nums[mid + 1]:  # Move toward the peak\n            left = mid + 1\n        else:  # Discard the right part\n            right = mid\n    return left\n\nSearching in a rotated sorted array:\n\ndef search_in_rotated_array(nums, target):\n    left, right = 0, len(nums) - 1\n    while left <= right:\n        mid = left + (right - left) // 2\n\n        # If target is found\n        if nums[mid] == target:\n            return mid\n\n        # Determine which side is sorted\n        if nums[left] <= nums[mid]:  # Left half is sorted\n            if nums[left] <= target < nums[mid]:\n                right = mid - 1  # Target is in the left half\n            else:\n                left = mid + 1  # Target is in the right half\n        else:  # Right half is sorted\n            if nums[mid] < target <= nums[right]:\n                left = mid + 1  # Target is in the right half\n            else:\n                right = mid - 1  # Target is in the left half\n\n    return -1  # Target not found\n\n5. Explanation\n\nMonotonic Property:\n\nIf nums[mid] < nums[mid + 1], the peak must lie in the right half.\n\nElse, it lies in the left half.\n\nTermination: left == right, pointing to the peak element.\n\nThis pattern efficiently handles problems where the array isn’t fully sorted, but binary search works due to conceptual or monotonic properties.\n\nHeaps\n\n1. Kth Largest/Smallest Element\n\nPattern Overview\n\nUse a min-heap for finding the kth largest element.\n\nUse a max-heap for finding the kth smallest element.\n\nThe idea is to maintain a heap of size k that tracks the desired elements efficiently.\n\nCommon Problems\n\nKth Largest Element in an Array.\n\nKth Smallest Element in a Sorted Matrix.\n\nKey Points\n\nMin-heap is used for the kth largest because the smallest element in the heap is replaced once the size exceeds k.\n\nMax-heap is used for the kth smallest by negating the values (since Python's heapq is a min-heap by default).\n\nExample Code\n\nimport heapq\n\n# Kth Largest Element in an Array\ndef findKthLargest(nums, k):\n    min_heap = []\n    for num in nums:\n        heapq.heappush(min_heap, num)\n        if len(min_heap) > k:\n            heapq.heappop(min_heap)\n    return min_heap[0]\n\n2. Top K Elements\n\nPattern Overview\n\nUse a heap to efficiently retrieve the top k elements based on custom criteria (e.g., frequency, value).\n\nCombine heap operations with frequency maps or sorted structures.\n\nCommon Problems\n\nTop K Frequent Elements.\n\nTop K Frequent Words.\n\nKey Points\n\nUse a max-heap if k elements need to be sorted in descending order.\n\nUse a min-heap to maintain a heap of size k.\n\nExample Code\n\n# Top K Frequent Elements\nfrom collections import Counter\nimport heapq\n\ndef topKFrequent(nums, k):\n    freq_map = Counter(nums)\n    min_heap = []\n    \n    for num, freq in freq_map.items():\n        heapq.heappush(min_heap, (freq, num))\n        if len(min_heap) > k:\n            heapq.heappop(min_heap)\n    \n    return [num for freq, num in min_heap]\n\n3. Merge K Sorted Lists/Arrays\n\nPattern Overview\n\nUse a min-heap to efficiently merge k sorted arrays or lists.\n\nThe heap is used to track the smallest element from each array, and the result is built incrementally.\n\nCommon Problems\n\nMerge K Sorted Lists.\n\nSmallest Range Covering Elements from K Lists.\n\nKey Points\n\nPush the first element of each list/array into the heap with an identifier (e.g., index).\n\nExtract the smallest element, add it to the result, and push the next element from the same list into the heap.\n\nExample Code\n\nimport heapq\n\n# Merge K Sorted Lists\ndef mergeKLists(lists):\n    min_heap = []\n    \n    # Push initial elements of each list into the heap\n    for i, lst in enumerate(lists):\n        if lst:\n            heapq.heappush(min_heap, (lst[0], i, 0))  # (value, list index, element index)\n    \n    result = []\n    while min_heap:\n        val, list_idx, elem_idx = heapq.heappop(min_heap)\n        result.append(val)\n        if elem_idx + 1 < len(lists[list_idx]):\n            heapq.heappush(min_heap, (lists[list_idx][elem_idx + 1], list_idx, elem_idx + 1))\n    \n    return result\n\n4. Two Heaps Pattern\n\nPattern Overview\n\nUse two heaps (max-heap and min-heap) to efficiently manage data in two halves.\n\nUseful for problems requiring median calculation or balancing data partitions.\n\nCommon Problems\n\nFind Median from Data Stream.\n\nSliding Window Median.\n\nKey Points\n\nUse a max-heap for the left half of the data and a min-heap for the right half.\n\nMaintain the size property:\n\nMax-heap can have at most one more element than the min-heap.\n\nExample Code\n\nimport heapq\n\n# Find Median from Data Stream\nclass MedianFinder:\n    def __init__(self):\n        self.small = []  # Max-heap for the smaller half (invert values for max-heap)\n        self.large = []  # Min-heap for the larger half\n    \n    def addNum(self, num):\n        heapq.heappush(self.small, -num)\n        heapq.heappush(self.large, -heapq.heappop(self.small))\n        \n        if len(self.small) < len(self.large):\n            heapq.heappush(self.small, -heapq.heappop(self.large))\n    \n    def findMedian(self):\n        if len(self.small) > len(self.large):\n            return -self.small[0]\n        return (-self.small[0] + self.large[0]) / 2.0\n\nLinked Lists\n\n1. Reversal-Based Pattern\n\nTemplate Code: Reverse a Linked List (Iterative)\n\ndef reverseList(head):\n    prev, curr = None, head\n    while curr:\n        next_node = curr.next\n        curr.next = prev\n        prev, curr = curr, next_node\n    return prev\n\nCommon Problems:\n\nReverse a Linked List.\n\nReverse Nodes in K-Groups.\n\nReverse Linked List II (Partial reversal).\n\n2. Delete a Node (Dummy Node Simplification)\n\nTemplate Code: Delete a Node in a Linked List\nDelete a node when the head or a dummy node simplifies pointer handling.\n\ndef deleteNode(node):\n    node.val = node.next.val\n    node.next = node.next.next\n\nCommon Problems:\n\nDelete Node in a Linked List (given node reference).\n\nRemove Nth Node from the End of List (use dummy node + 2 pointers).\n\n3. Fast and Slow Pointer Pattern\n\nTemplate Code: Detect a Cycle in a Linked List\n\ndef hasCycle(head):\n    slow, fast = head, head\n    while fast and fast.next:\n        slow, fast = slow.next, fast.next.next\n        if slow == fast:\n            return True\n    return False\n\nCommon Problems:\n\nLinked List Cycle Detection.\n\nFind the Middle of the Linked List.\n\nDetect Cycle and Return Starting Node.\n\nIntersection of Two Linked Lists (length adjustment with pointers).\n\n4. Dummy Node for Simplification\n\nTemplate Code: Merge Two Sorted Lists\n\ndef mergeTwoLists(l1, l2):\n    dummy = ListNode(0)\n    curr = dummy\n    while l1 and l2:\n        if l1.val < l2.val:\n            curr.next, l1 = l1, l1.next\n        else:\n            curr.next, l2 = l2, l2.next\n        curr = curr.next\n    curr.next = l1 or l2\n    return dummy.next\n\nCommon Problems:\n\nMerge Two Sorted Lists.\n\nAdd Two Numbers.\n\nPartition List.\n\nMerge K Sorted Lists (heap approach uses dummy node for simplicity).\n\n5. Linked List ListNode\n\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nTrees\n\n1. DFS Traversals (Recursive & Iterative)\n\nRecursive DFS Template:\n\ndef dfs(node):\n    if not node:\n        return\n    # Preorder logic (process node)\n    dfs(node.left)\n    # Inorder logic (process node)\n    dfs(node.right)\n    # Postorder logic (process node)\n\nIterative DFS (Inorder Example):\n\ndef dfs_iterative(root):\n    stack, result = [], []\n    while stack or root:\n        while root:\n            stack.append(root)\n            root = root.left\n        root = stack.pop()\n        result.append(root.val)  # Process node\n        root = root.right\n    return result\n\n2. BFS (Level Order Traversal)\n\nTemplate:\n\nfrom collections import deque\ndef bfs(root):\n    if not root:\n        return []\n    queue, result = deque([root]), []\n    while queue:\n        level = []\n        for _ in range(len(queue)):\n            node = queue.popleft()\n            level.append(node.val)  # Process node\n            if node.left: queue.append(node.left)\n            if node.right: queue.append(node.right)\n        result.append(level)\n    return result\n\n3. Tree Construction (Preorder + Inorder Example)\n\nTemplate:\n\ndef build_tree(preorder, inorder):\n    if not preorder or not inorder:\n        return None\n    root_val = preorder.pop(0)\n    root = TreeNode(root_val)\n    idx = inorder.index(root_val)\n    root.left = build_tree(preorder, inorder[:idx])\n    root.right = build_tree(preorder, inorder[idx+1:])\n    return root\n\nBinary Trees (Additional Patterns)\n\n1. Symmetric Tree:\n\nTemplate:\n\ndef is_symmetric(root):\n    def check(left, right):\n        if not left and not right:\n            return True\n        if not left or not right or left.val != right.val:\n            return False\n        return check(left.left, right.right) and check(left.right, right.left)\n    return check(root.left, root.right) if root else True\n\n2. Flatten Binary Tree to Linked List:\n\nTemplate:\n\ndef flatten(root):\n    def dfs(node):\n        if not node:\n            return None\n        left_tail = dfs(node.left)\n        right_tail = dfs(node.right)\n        if left_tail:\n            left_tail.right = node.right\n            node.right = node.left\n            node.left = None\n        return right_tail or left_tail or node\n    dfs(root)\n\n3. Lowest Common Ancestor (LCA):\n\nTemplate:\n\ndef lca(root, p, q):\n    if not root or root == p or root == q:\n        return root\n    left = lca(root.left, p, q)\n    right = lca(root.right, p, q)\n    return root if left and right else left or right\n\nBinary Search Trees (BST Patterns)\n\n1. Validate BST:\n\nTemplate:\n\ndef is_valid_bst(root):\n    def validate(node, low, high):\n        if not node:\n            return True\n        if not (low < node.val < high):\n            return False\n        return validate(node.left, low, node.val) and validate(node.right, node.val, high)\n    return validate(root, float('-inf'), float('inf'))\n\n2. Search in BST:\n\nTemplate:\n\ndef search_bst(root, val):\n    if not root or root.val == val:\n        return root\n    return search_bst(root.left, val) if val < root.val else search_bst(root.right, val)\n\n3. Kth Smallest Element in BST:\n\nTemplate:\n\ndef kth_smallest(root, k):\n    stack = []\n    while True:\n        while root:\n            stack.append(root)\n            root = root.left\n        root = stack.pop()\n        k -= 1\n        if k == 0:\n            return root.val\n        root = root.right\n\n4. Convert Sorted Array to BST:\n\nTemplate:\n\ndef sorted_array_to_bst(nums):\n    if not nums:\n        return None\n    mid = len(nums) // 2\n    root = TreeNode(nums[mid])\n    root.left = sorted_array_to_bst(nums[:mid])\n    root.right = sorted_array_to_bst(nums[mid+1:])\n    return root\n\nKey Notes:\n\nDFS/BFS: DFS is used for depth-based operations (e.g., paths, height), while BFS is ideal for level-based operations.\n\nTree Construction: Always rely on unique traversal pairs (e.g., Preorder + Inorder).\n\nBinary Trees: Focus on symmetry, serialization, and manipulation (invert/flatten).\n\nBST: Leverage sorted property for efficient search, validation, and construction.\n\nConcise Notes on Propagating Results in Recursion\n\nGlobal Best Answer: Use a global variable to track the best value across all nodes (e.g., max path sum, tree diameter). Subtree results help compute the local value, and the global variable holds the final answer.\n\nAggregate Subtree Results: Combine results from left and right subtrees to compute the parent’s value, which may also be the final answer (e.g., count nodes, check balance).\n\nIntermediate Results: Propagate meaningful results upward to aid parent computations (e.g., LCA, path sums). Use special values (e.g., -1) to signal specific states.\n\nGlobal State Updates: Use external variables to track cumulative or specific results during recursion (e.g., count of good subtrees).\n\nKey Idea: Identify whether the task requires combining subtree results, tracking a global best, or propagating intermediate states. Optimize by only returning necessary information.","favorited":false},{"name":"LLD","id":"885313a0-9acc-4873-b0b3-48d44b5a8ed4","color":"#999999","icon":"file-text","fileName":"LLD885313a0-9acc-4873-b0b3-4","textContent":"RoadMap\n\nClasses - Methods, Attributes, Class Variables, Class Methods 2) OOPS concepts - Abstraction, Polymorphism, Encapsulation, Inheritance 3) super() method 4) Design Principles - SOLID, DRY, KISS, YAGNI 5) Design Patterns - Only important ones - Singleton, Factory Method, Builder, Strategy, Observer 6) Importants concepts/classes to know - Unique ID generation, Desgining Rate Limiter (Token bucket, fixed window), Caching (LRU, LFU), Queue management (Asynchronous Processing of Tasks, Desiognning notifications Systems using message queue/observer patterns), time based schedulers like crons jobs, load balancing algorithms, Data Paginiation and limtiing results, Search and Fileting, Acces contro and permissions, Handling concurrency and synchronization\n\nImportant Questions to Practice:\n\nPizza Shop Question\n\nUnix/linux File search question\n\nLRU Cache (Indirect uses too), with TTL\n\nLFU Cache\n\nHotel Booking/Management System \n\nelevator system in a building\n\nParking Lot Design\n\nLoad Balancer\n\nFile Management System\n\nURL shortner\n\nIncome Calculator\n\nHashtable\n\nAmazon Locker System (Scalable storage locker)\n\nAmazon Package Delivery System\n\nAmazon Validation of a Purchase class\n\nDesign Calendar\n\nAbstract Data structure that supports find/add/retrieve/delete etc.\n\nCustom collection class - getRandome, add, retrieve etc. \n\nFind Connected id given list of id's (friends, their tastes etc.)\n\nHistorical search terms by frequency of search (Trie)\n\nAPI Gateway\n\nGames - Chess Game, Snake and Ladder, Wordle, Spell Bee, Sudoku etc.\n\nRate Limiter \n\nCommon LLD Questions - Library, Vending Machine, Amazon Online shopping System, Movie ticket booking etc. \n\nNotification System LLD\n\nMost frequent/vs least frequent in huge database - heap internals/Trie\n\nAutocomplete - Trie\n\nClasses and Methods\n\nClass Definition and Initialization\n\nClasses are the blueprint for creating objects in Python.\n\nclass MyClass:\n    def __init__(self, attribute1, attribute2):\n        self.attribute1 = attribute1  # Instance variable\n        self.attribute2 = attribute2\n\n# Creating an object\nobj = MyClass(\"value1\", \"value2\")\n\nInstance Variables and Methods\n\nInstance variables are specific to each object.\n\nInstance methods operate on instance variables.\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def greet(self):\n        return f\"Hello, my name is {self.name} and I am {self.age} years old.\"\n\n# Example usage\np = Person(\"Alice\", 25)\nprint(p.greet())  # Output: Hello, my name is Alice and I am 25 years old.\n\nClass Variables and Class Methods\n\nClass variables are shared among all instances of a class.\n\nClass methods operate on class variables and are denoted with @classmethod.\n\nclass Counter:\n    count = 0  # Class variable\n\n    @classmethod\n    def increment(cls):\n        cls.count += 1\n\n# Example usage\nCounter.increment()\nprint(Counter.count)  # Output: 1\n\nStatic Methods\n\nStatic methods do not operate on class or instance variables. They are utility functions within a class.\n\nclass MathUtils:\n    @staticmethod\n    def add(a, b):\n        return a + b\n\n# Example usage\nprint(MathUtils.add(5, 3))  # Output: 8\n\nOOP Concepts\n\nInheritance\n\nUse inheritance to create a hierarchy of classes.\n\nclass Animal:\n    def speak(self):\n        return \"I make a sound\"\n\nclass Dog(Animal):  # Dog inherits from Animal\n    def speak(self):\n        return \"Woof!\"\n\n# Example usage\ndog = Dog()\nprint(dog.speak())  # Output: Woof!\n\nEncapsulation\n\nUse private variables to restrict access to attributes.\n\nPrefix variables with _ (protected) or __ (private).\n\nclass BankAccount:\n    def __init__(self, balance):\n        self.__balance = balance  # Private variable\n\n    def get_balance(self):\n        return self.__balance\n\n# Example usage\naccount = BankAccount(100)\nprint(account.get_balance())  # Output: 100\n\nPolymorphism\n\nSame interface but different implementations.\n\nclass Animal:\n    def speak(self):\n        return \"Some sound\"\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\n# Example usage\nanimals = [Dog(), Cat()]\nfor animal in animals:\n    print(animal.speak())  # Output: Woof! Meow!\n\nAbstraction\n\nUse abstract classes to define methods that must be implemented in derived classes.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def area(self):\n        pass\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n# Example usage\nrect = Rectangle(5, 10)\nprint(rect.area())  # Output: 50\n\nSome Syntax Bits\n\nMethod Overloading (Default Arguments)\n\nPython doesn't support true method overloading, but you can simulate it using default arguments.\n\nclass Math:\n    def add(self, a, b=0, c=0):\n        return a + b + c\n\n# Example usage\nmath = Math()\nprint(math.add(5))        # Output: 5\nprint(math.add(5, 3))     # Output: 8\nprint(math.add(5, 3, 2))  # Output: 10\n\nMethod Overriding\n\nRedefine a method from the parent class in the child class.\n\nclass Parent:\n    def greet(self):\n        return \"Hello from Parent\"\n\nclass Child(Parent):\n    def greet(self):\n        return \"Hello from Child\"\n\n# Example usage\nchild = Child()\nprint(child.greet())  # Output: Hello from Child\n\nDesign Principles\n\nSOLID Principles\n\nSingle Responsibility Principle (SRP):\n\nDefinition: A class should have only one reason to change, meaning it should only have one job.\n\nExample:\n\nclass Invoice:\n    def __init__(self, items):\n        self.items = items\n\n    def calculate_total(self):\n        return sum(self.items)\n\nclass InvoicePrinter:\n    def print_invoice(self, invoice):\n        print(f\"Invoice Total: {invoice.calculate_total()}\")\n\nWhy: Separates the responsibilities of calculating and printing the invoice.\n\nOpen/Closed Principle (OCP):\n\nDefinition: A class should be open for extension but closed for modification.\n\nExample:\n\nclass Discount:\n    def apply_discount(self, price):\n        return price\n\nclass PercentageDiscount(Discount):\n    def __init__(self, percent):\n        self.percent = percent\n\n    def apply_discount(self, price):\n        return price * (1 - self.percent / 100)\n\nWhy: New discount types can be added by extending the Discount class without modifying existing code.\n\nLiskov Substitution Principle (LSP):\n\nDefinition: Subtypes must be substitutable for their base types.\n\nExample:\n\nclass Bird:\n    def fly(self):\n        pass\n\nclass Sparrow(Bird):\n    def fly(self):\n        print(\"Sparrow flying\")\n\ndef make_bird_fly(bird):\n    bird.fly()\n\nmake_bird_fly(Sparrow())  # Works correctly\n\nWhy: Subclasses like Sparrow can replace Bird without breaking functionality.\n\nInterface Segregation Principle (ISP):\n\nDefinition: A class should not be forced to implement interfaces it does not use.\n\nExample:\n\nclass Printer:\n    def print(self):\n        pass\n\nclass Scanner:\n    def scan(self):\n        pass\n\nclass AllInOneMachine(Printer, Scanner):\n    def print(self):\n        print(\"Printing...\")\n\n    def scan(self):\n        print(\"Scanning...\")\n\nWhy: The interfaces are split into Printer and Scanner, so classes only implement what they need.\n\nDependency Inversion Principle (DIP):\n\nDefinition: High-level modules should not depend on low-level modules. Both should depend on abstractions.\n\nExample:\n\nclass Database:\n    def save(self, data):\n        pass\n\nclass MySQLDatabase(Database):\n    def save(self, data):\n        print(\"Saving data to MySQL\")\n\nclass Application:\n    def __init__(self, database: Database):\n        self.database = database\n\n    def store_data(self, data):\n        self.database.save(data)\n\napp = Application(MySQLDatabase())\napp.store_data(\"Sample data\")\n\nWhy: The Application class depends on the abstraction Database, not on the concrete implementation.\n\nDRY (Don’t Repeat Yourself)\n\nDefinition: Avoid duplicating code by abstracting common functionality.\n\nExample:\n\ndef calculate_area(length, width):\n    return length * width\n\nclass Rectangle:\n    def __init__(self, length, width):\n        self.length = length\n        self.width = width\n\n    def area(self):\n        return calculate_area(self.length, self.width)\n\nKISS (Keep It Simple, Stupid)\n\nDefinition: Keep designs simple and avoid unnecessary complexity.\n\nExample:\n\ndef add(a, b):\n    return a + b\n\nAvoid overengineering a simple addition function.\n\nYAGNI (You Aren’t Gonna Need It)\n\nDefinition: Don’t implement functionality until it’s actually required.\n\nExample:\n\nclass Feature:\n    def useful_function(self):\n        print(\"This is useful now.\")\n\nAvoid adding speculative methods or features \"just in case\" they are needed later.\n\nDESIGN PATTERNS (Only Important Ones)\n\n1. Singleton Pattern\n\nPurpose: Ensures that a class has only one instance and provides a global access point to it.\n\nKey Use Cases: Configuration managers, database connections, logging systems.\n\nclass Singleton:\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n# Usage\ns1 = Singleton()\ns2 = Singleton()\nprint(s1 is s2)  # True\n\n2. Factory Method Pattern\n\nPurpose: Creates objects without specifying the exact class. Lets subclasses decide which class to instantiate.\n\nKey Use Cases: When a method must return objects of different types based on conditions.\n\nfrom abc import ABC, abstractmethod\n\nclass Notification(ABC):\n    @abstractmethod\n    def send(self, message):\n        pass\n\nclass EmailNotification(Notification):\n    def notify(self):\n        print(\"Sending Email Notification\")\n\nclass SMSNotification(Notification):\n    def notify(self):\n        print(\"Sending SMS Notification\")\n\nclass NotificationFactory:\n    @staticmethod\n    def create_notification(type_):\n        if type_ == \"email\":\n            return EmailNotification()\n        elif type_ == \"sms\":\n            return SMSNotification()\n        else:\n            raise ValueError(\"Invalid notification type\")\n\n# Usage\nfactory = NotificationFactory()\nnotification = factory.create_notification(\"email\")\nnotification.notify()  # Output: Sending Email Notification\n\n3. Builder Pattern\n\nPurpose: Constructs complex objects step by step, separating construction logic from the representation.\n\nKey Use Cases: Building objects with multiple optional parameters.\n\nclass House:\n    def __init__(self):\n        self.rooms = 0\n        self.garage = False\n        self.swimming_pool = False\n\n    def __str__(self):\n        return f\"House with {self.rooms} rooms, Garage: {self.garage}, Pool: {self.swimming_pool}\"\n\nclass HouseBuilder:\n    def __init__(self):\n        self.house = House()\n\n    def set_rooms(self, rooms):\n        self.house.rooms = rooms\n        return self\n\n    def add_garage(self):\n        self.house.garage = True\n        return self\n\n    def add_pool(self):\n        self.house.swimming_pool = True\n        return self\n\n    def build(self):\n        return self.house\n\n# Usage\nbuilder = HouseBuilder()\nluxury_house = builder.set_rooms(4).add_garage().add_pool().build()\nprint(luxury_house)  # Output: House with 4 rooms, Garage: True, Pool: True\n\n4. Strategy Pattern\n\nPurpose: Defines a family of algorithms, encapsulates each one, and makes them interchangeable at runtime.\n\nKey Use Cases: Dynamic algorithm selection, e.g., different payment methods.\n\nfrom abc import ABC, abstractmethod\n\nclass PaymentStrategy(ABC):\n    @abstractmethod\n    def pay(self, amount):\n        pass\n\nclass CreditCardPayment(PaymentStrategy):\n    def __init__(self, card_number, card_expiry):\n        self.card_number = card_number\n        self.card_expiry = card_expiry\n\n    def pay(self, amount):\n        print(f\"Paying ${amount} using Credit Card ending in {self.card_number[-4:]}\")\n\nclass PayPalPayment(PaymentStrategy):\n    def __init__(self, email):\n        self.email = email\n\n    def pay(self, amount):\n        print(f\"Paying ${amount} using PayPal account {self.email}\")\n\nclass BitcoinPayment(PaymentStrategy):\n    def __init__(self, wallet_address):\n        self.wallet_address = wallet_address\n\n    def pay(self, amount):\n        print(f\"Paying ${amount} using Bitcoin wallet {self.wallet_address}\")\n\nclass ShoppingCart:\n    def __init__(self):\n        self.items = []\n        self.total_amount = 0\n\n    def add_item(self, item, price):\n        self.items.append(item)\n        self.total_amount += price\n\n    def pay(self, payment_strategy: PaymentStrategy):\n        payment_strategy.pay(self.total_amount)\n\n# Create a shopping cart\ncart = ShoppingCart()\ncart.add_item(\"Book\", 15)\ncart.add_item(\"Pen\", 5)\n\n# Pay using different strategies\ncart.pay(CreditCardPayment(\"1234-5678-9012-3456\", \"12/23\"))\ncart.pay(PayPalPayment(\"user@example.com\"))\ncart.pay(BitcoinPayment(\"1A2b3C4d5E6f7G8h9I0J\"))\n\n5. Observer Pattern\n\nPurpose: Defines a one-to-many dependency where multiple observers are notified of changes in the subject’s state.\n\nKey Use Cases: Event-driven systems, like GUIs or notifications.\n\nclass Subject:\n    def __init__(self):\n        self._observers = []\n\n    def add_observer(self, observer):\n        self._observers.append(observer)\n\n    def remove_observer(self, observer):\n        self._observers.remove(observer)\n\n    def notify_observers(self, data):\n        for observer in self._observers:\n            observer.update(data)\n\nfrom abc import ABC, abstractmethod\n\nclass Observer:\n    @abstractmethod\n    def update(self, data):\n        pass\n\nclass EmailObserver(Observer):\n    def update(self, data):\n        print(f\"Email Observer: Received data - {data}\")\n\nclass SMSObserver(Observer):\n    def update(self, data):\n        print(f\"SMS Observer: Received data - {data}\")\n\n# Usage\nsubject = Subject()\nemail_observer = EmailObserver()\nsms_observer = SMSObserver()\n\nsubject.add_observer(email_observer)\nsubject.add_observer(sms_observer)\n\nsubject.notify_observers(\"System Update Available!\")\n# Output:\n# Email Observer: Received data - System Update Available!\n# SMS Observer: Received data - System Update Available!\n\n","favorited":false},{"name":"Random","id":"dd0b06c8-cc76-42e4-bba8-5fd595185473","color":"#999999","icon":"file-text","fileName":"Randomdd0b06c8-cc76-42e4-bba","textContent":"","favorited":false}],"opened":true}]}