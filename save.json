{"schema_version":1,"items":[{"name":"My Notes","id":"e04ba4d4-41ce-46cb-b5ee-5ec14a7ca79e","color":"#000000","icon":"book-2","children":[{"name":"README","id":"4ba4d441-ce36-4b35-ae5e-c14a7ca79e6a","color":"#999999","icon":"file-text","fileName":"Example-Note4ba4d441-ce36-4b","textContent":"Backtracking\n\nAlways sketch out the recursion tree in backtracking problems, solving becomes easy from there\n\nString Partitioning\n\nPartition a string into all possible substrings\n\nUses recursive calls to partition and check whatever valid condition needs to be checked (eg. is a palindrome, is present in a dictionary etc.)\n\nExample Problems \n\nProblem\n\nLink\n\nNotes\n\nPalindrome Partitioning\n\nhttps://leetcode.com/problems/palindrome-partitioning/description/\n\ncondition is palindrome\n\nWord Break II\n\nhttps://leetcode.com/problems/word-break-ii/description/\n\ncondition is present in dictionary\n\nresult = []\nn = len(s)\n\ndef backtrack(curr_partition, start):\n    if start == n:\n        result.append(curr_partition)\n        return\n    \n    for end in range(start + 1, n + 1):\n        substring = s[start:end]\n        if is_valid_substring(substring): #is_valid_substring changes with the type of problem at hand\n            curr_partition.append(substring)\n            backtrack(curr_partition, end) #Can also use backtrack(curr_partition + [substring], end)\n            curr_partition.pop()\n\nbacktrack([], 0)\nreturn result\n\ns: The input string to be partitioned.\n\nis_valid_substring: A function that checks if a given substring is valid (e.g., checks if it is a palindrome or if it is in a dictionary).\n\nresult: A list to store all the valid partitions.\n\nbacktrack Function: A helper function that performs the actual backtracking.\n\ncurr_partition: The current partition being constructed.\n\nstart: The starting index for partitioning the string.\n\nBase Case: If start reaches the end of the string (n), we add the current partition to the result.\n\nRecursive Case: We iterate through possible end indices (end), generate substrings from start to end, and check if they are valid using the is_valid_substring function. If valid, we recursively call backtrack with the new substring added to the current partition. After the recursive call, we backtrack by removing the last added substring.\n\nSubsets\n\nRecursively generate all subsets from a given set\n\nCan use 2 principles, 1st is for loop (so subsets starting with element) and 2nd is inclusion exclusion (Add curr to result only at the end)\n\nIt is better to use inclusion/exclusion when you want all subsets without any restrictions (combination sum, original subsets problem etc.), otherwise when you have restrictions (length, no duplicates etc.), it is better to use for loop strategy\n\nCan slightly modify the logic on what subsets to include based on any conditions (duplicates etc.)\n\nExample problems\n\nProblem\n\nLink\n\nNotes\n\nSubsets\n\nhttps://leetcode.com/problems/subsets/description/\n\nSubsets II\n\nhttps://leetcode.com/problems/subsets-ii/description/\n\nNo Duplicate Subsets\n\nCombinations\n\nhttps://leetcode.com/problems/combinations/description/\n\nSubsets of length k\n\nCombination Sum\n\nhttps://leetcode.com/problems/combination-sum/description/\n\nRepetition Allowed\n\nCombination Sum II\n\nhttps://leetcode.com/problems/combination-sum-ii/description/\n\nNo Repetition \n\nresult = []\nn = len(nums)\n\ndef backtrack(curr_subset, start):\n    result.append(list(curr_subset))\n    \n    for i in range(start, n):\n        curr_subset.append(nums[i]) # Add the current subset to the result\n        backtrack(curr_subset, i + 1) # Move to the next element\n        curr_subset.pop() # Exclude the current element (backtrack)\n\nbacktrack([], 0)\nreturn result\n\nresult = []\nn= = len(nums)\n\ndef backtrack(curr, i):\n    if i == n: # Base case: if we've considered all elements\n        result.append(curr)  \n        return\n\n    dfs(curr + [nums[i]], i + 1) # Inclusion: include nums[i] in the subset\n    dfs(curr, i + 1) # Exclusion: exclude nums[i] from the subset\n\nbacktrack([], 0)\nreturn result\n\nnums: The input list of numbers from which to generate subsets.\n\nresult: A list to store all the subsets.\n\nbacktrack Function: A helper function that performs the actual backtracking.\n\ncurr_subset: The current subset being constructed.\n\nstart: The starting index for the next element to consider.\n\nBase Case: Every time we call backtrack, we add the current subset (curr_subset) to the result.\n\nRecursive Case: We iterate through the elements starting from start to n, include the current element in the subset, and recursively call backtrack with the next starting index. After the recursive call, we backtrack by removing the last added element to explore other subsets.\n\nCode to identify non repeating elements in an array\n\nprev = None\n\nfor i in range(n):\n    if arr[i] != prev:\n        # Do something with the unique element 'arr[i]'\n        print(arr[i])  # Replace this with your desired operation\n    prev = arr[i]\n\n# Or easier, just use continue statement, if arr[i]==arr[i-1]: continue\n\nPermutations\n\nJust a slight variation of the combinations problem. Permutations generate all possible orders of elements in a given set. \n\nThe idea is to explore every possible order by fixing one element at a time and recursively permuting the remaining elements.\n\nExample problems:\n\nProblem\n\nLink\n\nNotes\n\nPermutations\n\nhttps://leetcode.com/problems/permutations/description/\n\nPermutations II\n\nhttps://leetcode.com/problems/permutations-ii/\n\nNo Duplicates\n\nresult = []\nn = len(nums)\n\ndef backtrack(curr_permutation, used):\n    if len(curr_permutation) == n: # Base case: if the current permutation is of length n, add it to result\n        result.append(list(curr_permutation))\n        return\n    \n    for i in range(n): \n        if used[i]: # Skip already used elements\n            continue\n\n        used[i] = True # Mark the current element as used\n        curr_permutation.append(nums[i]) \n        backtrack(curr_permutation, used) # Recurse with the updated permutation and used status\n        used[i] = False # Backtrack: unmark the element and remove it from the current permutation\n        curr_permutation.pop() \n\nbacktrack([], [False] * n)\nreturn result\n\n#You can maintain used, or just directly check if nums[i] is present in curr (this is O(n)), and if yes, just skip it. \n\nnums: The input list of numbers for which we want to generate permutations.\n\nresult: A list to store all the permutations.\n\nbacktrack Function: A helper function to perform the actual backtracking.\n\ncurr_permutation: The current permutation being constructed.\n\nused: A boolean list to keep track of which elements are used in the current permutation.\n\nBase Case: If the length of curr_permutation is equal to n, the current permutation is added to the result.\n\nRecursive Case: Iterate through the elements, check if the current element is used, and recursively generate permutations with the rest of the elements. After the recursive call, backtrack by marking the element as unused and removing it from the permutation.\n\nConstructing Valid Configurations\n\nConstruct valid solutions that adhere to specific constraints (like placing elements on a grid)\n\nExample Problems:\n\nProblem\n\nLink\n\nNotes\n\nN-Queens\n\nhttps://leetcode.com/problems/n-queens/description/\n\nSudoku Solver\n\nhttps://leetcode.com/problems/sudoku-solver/description/\n\ndef is_valid(inputs):\n    #Check the validity (n-queens is valid, sudoku board is valid etc.)\ndef backtrack(inputs):\n    \n    #the outer loops can changem this template is not 100% fitting, just for idea\n    #for loop (whatever you want to loop on)\n        if is_valid(inputs):\n            board[row][col] = 'Q'  # Place queen (In case of sudoku, its number)\n            #backtrack\n            board[row][col] = '.'  # Backtrack (remove queen/number)\n\nbacktrack(inputs)\nreturn result\n\nNote: Have to write code templates for dynamic programming + backtracking, DFS/BFS + backtracking. Mostly will be covered in DP and graph subsections. \n\nNote: There's also problems like valid parentheses, that don't fall into any of the patterns listed above, but it's just general backtracking and knowing when to stop (opening brackets >= closing brackets)\n\nGraphs\n\nMost leetcode problems in graphs are either adjacency lists, or grid based problems. Occasionally, you might encounter graphs represented using Nodes, and graphs represented using adjacency matrix. \n\nTry to pass both the graph, visited set as arguments to the function, as in Python, only references to mutable objects are passed, so it is the same object, not a copy. \n\nConverting edges to adjacency list\n\ndef edge_list_to_adj_list(edges: list, n: int):\n    # Create an empty adjacency list with default as an empty list\n    adj_list = defaultdict(list)\n    \n    # Iterate over the edge list to populate the adjacency list\n    for u, v in edges:\n        adj_list[u].append(v)\n        adj_list[v].append(u)  # If the graph is undirected, add both ways\n    \n    return adj_list\n\nDFS vs BFS\n\nDFS:\n\nMark node as visited: When popped from the stack (ready to process). \n\nWhy?: Ensures full exploration of neighbors before marking as visited.\n\nAdditional visited check: Needed before pushing neighbors onto the stack to avoid pushing the same node multiple times (because DFS might revisit nodes from different branches). (Only in case of iterative)\n\nRecursive DFS:\n\nNo need for an additional visited check because the recursion stack inherently manages depth-first traversal, and nodes are marked as visited immediately when the function is called.\n\nThe call stack prevents revisiting nodes by the nature of recursion.\n\nIn recursive DFS, you process the node first, recursively call neighbors, and only after all recursive calls are done does the node \"pop\" from the stack (when the function returns).\n\nIterative DFS:\n\nRequires two visited checks:\n\nBefore pushing neighbors onto the stack, to avoid pushing already visited nodes.\n\nBefore processing the node after popping from the stack, to ensure the node is only processed once, even if it's added to the stack multiple times from different paths.\n\nIn iterative DFS, you pop the node first, process it, then push neighbors to the stack.\n\nThe difference arises from how the call stack in recursion automatically manages depth-first exploration compared to manual stack management in iterative DFS, so don't worry too much, just memorize. \n\nBFS:\n\nMark node as visited: When enqueued (immediately after adding to the queue).\n\nWhy?: BFS processes nodes level by level, so marking when enqueuing prevents revisiting and ensures the shortest path is maintained.\n\nNo additional visited check: Since nodes are marked visited when enqueued, they won’t be added to the queue again, making an additional check after dequeuing unnecessary.\n\nKey Points:\n\nDFS explores deeply; multiple paths might push the same node, so check before pushing.\n\nBFS explores level by level; mark when enqueuing to ensure each node is processed once, in the correct order.\n\nEfficiency: BFS marking on enqueue is optimal for reducing redundant checks.\n\nDFS\n\nDFS magic spell: 1]push to stack, 2] pop top , 3] retrieve unvisited neighbours of top, push them to stack 4] repeat 1,2,3 while stack not empty. Now form a rap !\n\nRecursive DFS (adjacency list)\n\ndef dfs(node: int, visited: set, graph: dict):\n    # Mark the current node as visited\n    visited.add(node)\n    \n    # Process the current node (e.g., print, collect data, etc.)\n    print(f\"Visiting node {node}\")\n    \n    # Recursively visit all unvisited neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            dfs(neighbor, visited, graph)\n\nRecursive DFS (grid)\n\ndef dfs(x: int, y: int, visited: set, grid: list):\n    # Mark the current cell as visited\n    visited.add((x, y))\n    \n    # Process the current cell (e.g., print, collect data, etc.)\n    print(f\"Visiting cell ({x}, {y})\")\n    \n    # Define the directions for neighbors: up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Get the grid dimensions\n    rows, cols = len(grid), len(grid[0])\n    \n    # Recursively visit all unvisited neighbors within bounds\n    for dx, dy in directions:\n        nx, ny = x + dx, y + dy\n        if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited: #Here you can add additional conditions, like edge exists only if it is a 1 etc.\n            dfs(nx, ny, visited, grid)\n\nIterative DFS (adjacency list)\n\nWhen you push a node onto the stack, you're only checking if it's not visited yet at that moment. However, the same node might get added to the stack multiple times through different paths before it is actually processed. Thats the reason why we check visited both at the beginning and before adding neighbor. Think 1 - 2 - 3 - 4 loop. \n\nvisited = set()  # To track visited nodes\nstack = [start]  # Initialize the stack with the starting node\n\nwhile stack:\n    node = stack.pop()  # Pop the last node added (LIFO order)\n\n    if node not in visited:\n        # Mark the node as visited\n        visited.add(node)\n        print(f\"Visiting node {node}\")\n\n        # Push all unvisited neighbors onto the stack\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                stack.append(neighbor)\n\nIterative DFS (grid)\n\nvisited = set()  # To track visited cells\nstack = [(start_x, start_y)]  # Initialize the stack with the starting cell\n\n# Define the directions for neighbors: up, down, left, right\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n# Get the grid dimensions\nrows, cols = len(grid), len(grid[0])\n\nwhile stack:\n    x, y = stack.pop()  # Pop the last cell added\n\n    if (x, y) not in visited:\n        # Mark the current cell as visited\n        visited.add((x, y))\n        print(f\"Visiting cell ({x}, {y})\")\n\n        # Push all unvisited neighbors onto the stack (within bounds)\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:\n                stack.append((nx, ny))\n\nRecursive DFS to keep track of Path (adjacency list)\n\nThis works both in cyclic and acyclic graphs, as in the backtracking step, we are removing the node from the visited set once we finish exploring its neighbors. This prevents the algorithm from getting stuck in an infinite loop caused by cycles while still allowing revisits to nodes in different paths.\n\ndef dfs(node, target, graph, visited, path, all_paths):\n    visited.add(node)\n    path.append(node)\n    \n    if node == target:\n        # If we've reached the target, store the current path\n        all_paths.append(list(path))\n    else:\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                dfs(neighbor, target, graph, visited, path, all_paths)\n    \n    path.pop()  # Backtrack\n    visited.remove(node)\n\nRecursive DFS to keep track of Path (grid)\n\ndef dfs(x, y, target_x, target_y, grid, visited, path, all_paths):\n    # Add current cell to the path and mark it as visited\n    path.append((x, y))\n    visited.add((x, y))\n    \n    # If we reach the target cell, store the current path\n    if (x, y) == (target_x, target_y):\n        all_paths.append(list(path))  # Store a copy of the path\n    else:\n        # Explore the 4 possible directions: up, down, left, right\n        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n        rows, cols = len(grid), len(grid[0])\n\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            # Check if the next cell is within bounds and not visited\n            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited and grid[nx][ny] == 1:\n                dfs(nx, ny, target_x, target_y, grid, visited, path, all_paths)\n    \n    # Backtrack: remove the current cell from the path and unmark it as visited\n    path.pop()\n    visited.remove((x, y))\n\nRecursive DFS for topological sort (Directed Graph)\n\nKey point is, Once all neighbors of the current node have been processed, the current node is added to the stack.\n\nAfter performing DFS on all unvisited nodes, the stack will contain the nodes in reverse topological order (because nodes are pushed to the stack after their dependencies have been processed).\n\nResult: The topological order is obtained by reversing the stack.\n\nSome important points: This code will only work if there is no cycle, i.e , incase of a DAG. If you want it to work even when cycles are there, and return empty array if cycles are there, you need to add cycle detection logic. \n\ndef dfs_topological(node, graph, visited, stack):\n    visited.add(node)  # Mark the current node as visited\n\n    # Recursively visit all unvisited neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            dfs_topological(neighbor, graph, visited, stack)\n    \n    # After all neighbors are processed, add the current node to the stack\n    stack.append(node)\n\nvisited = set()  # Set to keep track of visited nodes\nstack = []  # Stack to store the topological order\n\n# Perform DFS from every node to ensure all nodes are visited\nfor node in range(n):\n    if node not in visited:\n        dfs_topological(node, graph, visited, stack)\n\n# The topological order is the reverse of the DFS post-order traversal\nreturn stack[::-1] \n\nRecursive DFS for Cycle Detection (Directed Graph)\n\nCycle detection is based on the Key point: In the current path, if there is back edge, i.e, node connecting to any previous nodes only in the current path, there is a cycle. \n\nYou cannot use visited to keep track of cycles, i.e claim that if we revisit the node there is a cycle, as a node maybe visited multiple times in DFS. \n\nYou also can't check something like if node in recursion_stack at the very beginning, because we will never visit the same node again due to us keeping track of visited. So that statement would never be True. So we always have to keep the main logic as detecting back edge. \n\ndef dfs_cycle(node, graph, visited, recursion_stack):\n    visited.add(node)  # Mark the node as visited\n    recursion_stack.add(node)  # Add the node to the current recursion stack\n\n    # Explore the neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            if dfs_cycle(neighbor, graph, visited, recursion_stack):\n                return True  # Cycle detected (If you don't do this, True won't be propogated)\n        elif neighbor in recursion_stack:\n            return True  # Cycle detected (back edge found)\n\n    # Backtrack: remove the node from the recursion stack\n    recursion_stack.remove(node)\n    return False\n\nBFS\n\nBFS (adjacency list)\n\nvisited = set()  # To track visited nodes\nqueue = deque([start])  # Initialize the queue with the starting node\nvisited.add(start)  # Mark the start node as visited when enqueuing\n\nwhile queue:\n    node = queue.popleft()  # Dequeue the first node in the queue\n    print(f\"Visiting node {node}\")  # Process the node (e.g., print or collect data)\n\n    # Enqueue all unvisited neighbors and mark them as visited when enqueuing\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            queue.append(neighbor)\n            visited.add(neighbor)  # Mark as visited when enqueuing\n\nBFS (grid)\n\nvisited = set()  # To track visited cells\nqueue = deque([(start_x, start_y)])  # Initialize the queue with the starting cell\nvisited.add((start_x, start_y))  # Mark the start cell as visited when enqueuing\n\n# Define the directions for neighbors: up, down, left, right\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\nrows, cols = len(grid), len(grid[0])\n\nwhile queue:\n    x, y = queue.popleft()  # Dequeue the first cell\n    print(f\"Visiting cell ({x}, {y})\")  # Process the current cell\n\n    # Enqueue all unvisited valid neighbors\n    for dx, dy in directions:\n        nx, ny = x + dx, y + dy\n        if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:\n            queue.append((nx, ny))\n            visited.add((nx, ny))  # Mark as visited when enqueuing\n\nMultisource BFS\n\nThe multi-source BFS pattern is useful when you need to start BFS from multiple starting points simultaneously. This pattern ensures that all sources are explored in parallel, and it's commonly used in problems like finding the shortest distance from multiple sources to a destination.\n\nThe only change is from normal BFS code is that you add all the source nodes in the queue and call BFS\n\nMaintaining Level information in BFS\n\nSimple way is just to maintain (node, level) instead of just node. Each time you are enqueuing new nodes, increment the level by 1. This way, you have level information for all the nodes. In this method, level information will be lost at the end, as the queue will become empty. It can still be used if you only need the end result, but if you need information like no. of nodes in each level etc., It is better to use level processing approach. \n\nOther way is using array for levels, like so. Idea is to process nodes level by level, tracking the current level by processing all nodes at the same depth in one batch, and incrementing the level after processing each layer. Useful in tree problems (level order traversal) too.\n\nvisited = set([start])  # Track visited nodes, starting with the source node\nqueue = deque([start])  # Queue to store nodes to be processed\nlevel = 0  # Start from level 0 (the level of the start node)\n\nwhile queue:\n    # Get the number of nodes at the current level\n    level_size = len(queue)\n    \n    # Process all nodes at the current level\n    for _ in range(level_size):\n        node = queue.popleft()  # Pop a node from the queue\n        print(f\"Node: {node}, Level: {level}\")\n        \n        # Add unvisited neighbors to the queue\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n    \n    # After processing all nodes at the current level, increment the level\n    level += 1\n\nBFS for topological sort (Cycle detection built in) (Kahn's algorithm)\n\ngraph = defaultdict(list)  # Adjacency list representation of the graph\nin_degree = [0] * n  # In-degree of each node\n\n# Build the graph and calculate in-degrees\nfor start, end in edges:\n    graph[start].append(end)\n    in_degree[end] += 1\n\n# Initialize the queue with all nodes that have in-degree of 0\nqueue = deque([i for i in range(n) if in_degree[i] == 0])\ntopo_order = []\n\nwhile queue:\n    node = queue.popleft()  # Get the node with zero in-degree\n    topo_order.append(node)  # Add it to the topological order\n    \n    # Reduce in-degree of all its neighbors\n    for neighbor in graph[node]:\n        in_degree[neighbor] -= 1\n        # If a neighbor now has in-degree of 0, add it to the queue\n        if in_degree[neighbor] == 0:\n            queue.append(neighbor)\n\n# If all nodes are processed, return the topological order, otherwise return empty (cycle detected)\nif len(topo_order) == n:\n    return topo_order\nelse:\n    return []  # Cycle detected\n\nEulerian Path/Cycle\n\nEulerian Path: If there is exactly one vertex with out-degree greater by 1 and one with in-degree greater by 1.\n\nEulerian Cycle: If all vertices have equal in-degree and out-degree\n\nBasically Eulerian Path/Cycle means we cover all edges of a graph exactly once \n\nThe algorithm is simple, we recursively keep removing edges one by one, so no need of visited set as we can revisit a node multiple times, but cannot revisit an edge. More elegant implementations also exist, but this should suffice for this rare problem. \n\ndef dfs_eularian(node, graph, stack):\n    while graph[node]:\n        next_node = graph[node].pop(0) # Only if lexcial order pop(0), else its fine to pop any neighbor\n        dfs_eularian(next_node)\n    stack.append(node)\n\n# Start DFS from the determined starting airport\ndfs_eularian(start) # For determining start node, follow the instructions in the notes above.\nreturn stack[::-1]  # Reverse the itinerary to get the correct order\n\nDisjoint Set Union / Union Find\n\nPurpose: DSU is used to manage and merge disjoint sets, mainly in graph problems for tracking connected components and detecting cycles.\n\nKey Operations:\n\nFind with Path Compression: Reduces the time complexity by flattening the tree, so future operations are faster.\n\nUnion by Rank/Size: Keeps the tree balanced by attaching the smaller tree under the root of the larger one.\n\nTime Complexity: Both find and union have nearly constant time complexity, due to path compression and union by rank\n\nCommon Use Cases:\n\nCycle Detection in undirected graphs.\n\nKruskal’s MST Algorithm to avoid cycles when adding edges.\n\nConnected Components to check if two nodes are in the same component.\n\nInitialization: Use two arrays—parent (each node points to itself initially) and rank (initially 0 for all nodes).\n\nclass DSU:\n    def __init__(self, n):\n        # Initialize parent and rank arrays\n        self.parent = [i for i in range(n)]\n        self.rank = [0] * n\n\n    def find(self, x):\n        '''\n        # Intialize parent and rank as dicts {} if no. of nodes is not known, and just add these lines of code\n        # Initialize parent and rank if node is encountered for the first time\n        if x not in self.parent:\n            self.parent[x] = x\n            self.rank[x] = 0\n        '''\n        # Find the root of the set containing x with path compression\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n\n    def union(self, x, y):\n        # Union by rank\n        root_x = self.find(x)\n        root_y = self.find(y)\n\n        if root_x != root_y:\n            # Attach smaller rank tree under root of the higher rank tree\n            if self.rank[root_x] > self.rank[root_y]:\n                self.parent[root_y] = root_x\n            elif self.rank[root_x] < self.rank[root_y]:\n                self.parent[root_x] = root_y\n            else:\n                self.parent[root_y] = root_x\n                self.rank[root_x] += 1\n\n    def connected(self, x, y):\n        # Check if two elements are in the same set\n        return self.find(x) == self.find(y)\n\nMinimum Spanning Trees (MST)\n\nAn MST connects all nodes in an undirected, weighted graph with the minimum possible total edge weight, ensuring there are no cycles and the graph remains fully connected.\n\nMST Kruskal's\n\nApproach: Edge-based, Greedy\n\nProcess:\n\nSort all edges in non-decreasing order by weight.\n\nInitialize an empty MST and start adding edges from the sorted list.\n\nFor each edge, check if it forms a cycle using DSU. If not, add it to the MST.\n\nRepeat until the MST has V-1 exactly edges (where V is the number of vertices).\n\nBest for: Sparse graphs where sorting edges is manageable.\n\ndef kruskal_mst_fixed(edges):\n    # Initialize DSU and collect unique nodes to determine number of nodes\n    dsu = DynamicDSU()\n    unique_nodes = set(u for u, v, _ in edges).union(set(v for u, v, _ in edges))\n    num_nodes = len(unique_nodes)\n\n    # Sort edges by weight (ascending order)\n    edges.sort(key=lambda x: x[2])\n\n    mst = []  # To store edges in MST\n    total_cost = 0\n\n    # Iterate through sorted edges\n    for u, v, weight in edges:\n        # Only add edge if it doesn't form a cycle\n        if not dsu.connected(u, v):\n            dsu.union(u, v)  # Union the two vertices\n            mst.append((u, v, weight))  # Add edge to MST\n            total_cost += weight  # Add edge weight to total cost\n\n            # Stop if MST has enough edges (n - 1 edges for n nodes)\n            if len(mst) == num_nodes - 1:\n                break\n\n    return mst, total_cost\n\nMST Prim's\n\nApproach: Vertex-based, Greedy\n\nProcess:\n\nStart from any initial node.\n\nUse a min-heap (priority queue) to track edges that extend from the MST.\n\nRepeatedly pop the minimum edge from the heap:\n\nIf it connects to an unvisited node, add it to the MST and add its neighbors to the heap.\n\nStop when the MST includes all nodes.\n\nBest for: Dense graphs with adjacency lists/matrices.\n\n# Redefining Prim's algorithm for MST with adjacency list input\ndef prim_mst(graph, start):\n    # Initialize structures\n    mst = []  # To store the edges in the MST\n    total_cost = 0  # To accumulate the total weight of the MST\n    visited = set()  # Track nodes already included in the MST\n    min_heap = []  # Priority queue (min-heap) for edges\n\n    # Function to add edges to the priority queue\n    def add_edges(node):\n        visited.add(node)\n        for neighbor, weight in graph[node]:\n            if neighbor not in visited:\n                heapq.heappush(min_heap, (weight, node, neighbor))\n\n    # Start from the initial node\n    add_edges(start)\n\n    # Process until MST includes all nodes or min-heap is empty\n    while min_heap and len(visited) < len(graph):\n        weight, u, v = heapq.heappop(min_heap)\n        if v not in visited:  # Only add edge if it connects to an unvisited node\n            mst.append((u, v, weight))\n            total_cost += weight\n            add_edges(v)  # Add edges from the newly added node\n\n    return mst, total_cost\n\nShortest Path Dijkstra\n\nApproach: Single-source shortest path for non-negative weights\n\nProcess:\n\nInitialize distances from the start node to all other nodes as infinity (except for the start, set to 0).\n\nUse a min-heap to manage nodes by their current shortest distance.\n\nPop the node with the smallest distance:\n\nFor each neighbor, calculate the potential new distance.\n\nIf this distance is shorter than the known distance, update it and push the neighbor with the updated distance.\n\nContinue until all reachable nodes have the shortest path from the start.\n\nBest for: Shortest paths in non-negative weighted graphs.\n\ndef dijkstra(graph, start):\n    # Initialize distance dictionary with infinity for all nodes except the start\n    distances = {node: float('inf') for node in graph}\n    distances[start] = 0\n\n    # Priority queue (min-heap) initialized with the starting node\n    min_heap = [(0, start)]  # (distance, node)\n\n    while min_heap:\n        # Pop the node with the smallest distance\n        current_distance, u = heapq.heappop(min_heap)\n\n        # Process only if the current distance is the smallest known distance for u\n        if current_distance > distances[u]:\n            continue\n\n        # Check each neighbor of the current node\n        for v, weight in graph[u]:\n            distance = current_distance + weight  # Calculate potential new distance to neighbor\n\n            # Only consider this path if it's shorter than the known distance\n            if distance < distances[v]:\n                distances[v] = distance  # Update to the shorter distance\n                heapq.heappush(min_heap, (distance, v))  # Push updated distance into the heap\n\n    return distances\n\nTips and Tricks to Solve graph problems\n\nTo detect length of cycle or elements in cycle, you can keep track of entry times in the recursive_stack. This can also help you in finding the exact cycle. \n\nFor undirected graphs, cycles are found using DSU or DFS with back edges. For directed graphs, cycles are detected through DFS with recursion stack tracking.\n\nDynamic Programming\n\nGeneral tip - In bottom up DP, if 2D DP, draw the matrix and visualize the dependencies, becomes easier.\n\nSometime memoization is more intuitive, sometime DP is more intuitive. DP you can usually perform space optimization. \n\n0/1 Knapsack Pattern - \n\ndef knapsack(values, weights, capacity):\n    n = len(values)\n    dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n    \n    # Fill the DP table\n    for i in range(1, n + 1):  # For each item\n        for w in range(1, capacity + 1):  # For each capacity\n            if weights[i - 1] <= w:\n                dp[i][w] = max(dp[i - 1][w], values[i - 1] + dp[i - 1][w - weights[i - 1]]) #i-1 is the actual item\n            else:\n                dp[i][w] = dp[i - 1][w]\n    \n    return dp[n][capacity]\n\nUnbounded Knapsack Patten - 322, 343, 279\n\ndef unbounded_knapsack(values, weights, capacity):\n    n = len(values)\n    dp = [0] * (capacity + 1)\n    \n    # Fill the DP array\n    for i in range(n):  # For each item\n        for w in range(weights[i], capacity + 1):  # For each capacity\n            dp[w] = max(dp[w], values[i] + dp[w - weights[i]])\n    \n    return dp[capacity]\n\nCoin Change II - Since ordering doesnt matter, it is a 2 state problem instead of 1 state. little tricky to catch. You use inclusion/exclusion decision tree, memoization solution is simple to implement. \n\nIf ordering does matter, then it is a simple 1 state solution like Coin Change I\n\nFibonacci\n\nThe Fibonacci pattern shows up in many dynamic programming problems where each state depends on a fixed number of previous states.\n\ndef fibonacci_dp(n):\n    if n <= 1:\n        return n\n    dp = [0] * (n + 1)\n    dp[1] = 1\n    for i in range(2, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\nLongest Palindromic Substring\n\nIf you know that a substring s[l+1:r-1] is a palindrome, then s[l:r] is also a palindrome if s[l] == s[r].\n\nIn problems involving palindromic substrings or subsequences, the goal is often to:\n\nIdentify the longest palindromic substring (continuous sequence).\n\nCount the number of palindromic substrings.\n\nFind the longest palindromic subsequence (which doesn’t need to be contiguous).\n\nImportant: Diagonal Filling of the matrix\n\ndef longestPalindrome(self, s: str) -> str:\nn= len(s)\ndp = [[False]*n for _ in range(n)]\n\nfor i in range(n):\n    dp[i][i] = True\nans = [0,0]\n\nfor i in range(n - 1):\n    if s[i] == s[i + 1]:\n        dp[i][i + 1] = True\n        ans = [i, i + 1]\n\nfor i in range(n-1, -1, -1):\n    for j in range(n-1, -1, -1):\n        if j<=i+1:\n            continue\n        if s[i]==s[j] and dp[i+1][j-1]:\n            dp[i][j] = True\n            if j-i>ans[1]-ans[0]:\n                ans = [i,j]\nreturn s[ans[0]:ans[1]+1]\n\nMaximum Sum/Product Subarrays\n\ndef max_subarray_sum(nums):\n    max_sum = nums[0]\n    current_sum = nums[0]\n    \n    for i in range(1, len(nums)):\n        current_sum = max(nums[i], current_sum + nums[i])\n        max_sum = max(max_sum, current_sum)\n    \n    return max_sum\n\ndef maxProduct(nums):\n        prev_max, prev_min = nums[0], nums[0]\n        ans=prev_max\n        for i in range(1,len(nums)):\n            curr_max = max(nums[i], nums[i]*prev_max, nums[i]*prev_min)\n            curr_min = min(nums[i], nums[i]*prev_max, nums[i]*prev_min)\n            prev_max, prev_min = curr_max, curr_min\n            ans = max(ans, prev_max)\n        return ans\n\nWord Break\n\n#O(n^2), dp[i] represents if word till i can be broken into parts. Important problem as you need to check all previous indices. \n\nLongest Increasing Subsequence\n\nThe Longest Increasing Subsequence (LIS) Pattern is a common dynamic programming pattern used to find subsequences within a sequence that meet certain increasing criteria. This pattern typically involves identifying or counting subsequences (not necessarily contiguous) that satisfy conditions related to increasing order, longest length, or specific values.\n\nDefine the DP Array: Use an array dp where dp[i] represents the length of the longest increasing subsequence ending at index i.\n\nTransition: For each element i, check all previous elements j < i. If nums[j] < nums[i], update dp[i] = max(dp[i], dp[j] + 1) to extend the subsequence ending at j.\n\nImportant: O(nlogn) Use tails to store the smallest ending of increasing subsequences. For each num, use binary search to find its position in tails – replace if within bounds, or append if beyond (is the last element)\n\ndef length_of_lis(nums):\n    \n    dp = [1] * len(nums)  # Each element is at least an increasing subsequence of length 1\n    \n    for i in range(1, len(nums)):\n        for j in range(i):\n            if nums[j] < nums[i]:\n                dp[i] = max(dp[i], dp[j] + 1)\n    \n    return max(dp)\n\nCounting Paths/Combinations Pattern\n\nGoal: Given a target, find distinct ways to reach it based on given moves/rules.\n\nDP Array/Table: Use dp[i] or dp[i][j] to store the count of ways to reach each target or cell.\n\nCommon Formula: For each i, update dp[i] by summing counts from preceding states based on allowed moves.\n\nKey Examples\n\nClimbing Stairs: dp[i] = dp[i-1] + dp[i-2]\n\nCoin Change (Combinations): dp[i] += dp[i - coin] for each coin\n\nGrid Unique Paths: dp[i][j] = dp[i-1][j] + dp[i][j-1]\n\nLongest Common Subsequence (LCS) Pattern\n\nGoal: Compare two sequences and find:\n\nLength of Longest Common Subsequence (LCS).\n\nMinimum edits to transform one sequence into another (Edit Distance).\n\nLongest contiguous substring (Longest Common Substring).\n\nKey Idea: Use a 2D DP table dp[i][j] where:\n\ndp[i][j] represents the result for substrings s1[:i] and s2[:j].\n\nBase Cases: If i == 0 or j == 0, the result is 0 (empty string).\n\ndef longest_common_subsequence(text1, text2):\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i - 1] == text2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n    return dp[m][n]\n\nVariants\n\nEdit Distance: Modify dp[i][j] transition to consider insert, delete, substitute. Also modify to correct base case. \n\ndp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+(1 if s1[i-1]!=s2[j-1] else 0))\n\nLongest Common Substring: If characters match, add +1 to previous diagonal:\n\nif text1[i - 1] == text2[j - 1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = 0\n\nState based DP Pattern\n\nGoal: Optimize decisions across states influenced by actions (e.g., buying/selling, resting/working).\n\nKey Idea: Define states to track situations (e.g., holding stock, not holding, cooldown) and transition equations between states.\n\nSteps:\n\nIdentify states based on possible actions.\n\nDraw a state diagram to visualize transitions.\n\nWrite recurrence relations for each state based on dependencies.\n\nBase Cases: Define initial conditions (e.g., starting with no stock or zero profit).\n\ndef state_based_dp_problem(prices):\n    # Base cases (initial states)\n    hold = -prices[0]   # Max profit when holding stock on day 0\n    not_hold = 0        # Max profit when not holding stock on day 0\n    cooldown = 0        # Max profit in cooldown on day 0\n\n    for i in range(1, len(prices)):\n        prev_hold = hold\n        # State transitions\n        hold = max(hold, not_hold - prices[i])        # Continue holding or buy today\n        not_hold = max(not_hold, cooldown)           # Continue not holding or end cooldown\n        cooldown = prev_hold + prices[i]             # Sell today and enter cooldown\n\n    # Final result: max profit can be in not_hold or cooldown\n    return max(not_hold, cooldown)\n\nTodo\n\nNegative marking \n\nPrefix Sum + Hashmap pattern (also modulo if involved)\n\nSliding window + Hashmap (also using matched variable to avoid hashmap)\n\nMonotonic Stack\n\nQuickSelect Algorithm \n\nStoring index as key in hashmap\n\nArrays + Hashmaps\n\nQuickSelect \n\nUse Cases:\n\nKth largest/smallest element or Top K elements in O(n) average time.\n\nAvoids full sorting for subset problems (better than nlogn).\n\nKey Insight:\n\nPartition around a pivot to partially sort: left satisfies the comparator, pivot lands in its correct position.\n\nHow to Use:\n\nSet k = k - 1 for 0-based indexing.\n\nUse x >= y for descending order (Kth largest, Top K).\n\nUse x <= y for ascending order (Kth smallest, Bottom K).\n\ndef quickselect(arr, left, right, k, comparator):\n    def partition(arr, left, right):\n        pivot = arr[right]  # Choose the last element as pivot\n        p = left  # Pointer for elements satisfying comparator\n        for i in range(left, right):\n            if comparator(arr[i], arr[right]):  # Compare with pivot\n                arr[i], arr[p] = arr[p], arr[i]\n                p += 1\n        arr[p], arr[right] = arr[right], arr[p]  # Place pivot in position\n        return p\n\n    if left <= right:\n        pivot_index = partition(arr, left, right)\n        if pivot_index == k:  # Found the Kth element\n            return arr[pivot_index]\n        elif pivot_index < k:  # Look for Kth in the right part\n            return quickselect(arr, pivot_index + 1, right, k, comparator)\n        else:  # Look for Kth in the left part\n            return quickselect(arr, left, pivot_index - 1, k, comparator)\n\n# Comparator for kth largest and top k elements \ncomparator = lambda x, y: x >= y\n\n# Comparator for kth smallest and bottom k elements \ncomparator = lambda x, y: x <= y\n\n","favorited":false},{"name":"LLD","id":"885313a0-9acc-4873-b0b3-48d44b5a8ed4","color":"#999999","icon":"file-text","fileName":"LLD885313a0-9acc-4873-b0b3-4","textContent":"RoadMap\n\nClasses - Methods, Attributed, Class Variables, Class Methods 2) OOPS concepts - Abstraction, Polymorphism, Encapsulation, Inheritance 3) super() method 4) Design Principles - SOLID, DRY, KISS, YAGNI 5) Design Patterns - Only important ones - Singleton, Factory Method, Builder, Strategy, Observer 6) Importants concepts/classes to know - Unique ID generation, Desgining Rate Limiter (Token bucket, fixed window), Caching (LRU, LFU), Queue management (Asynchronous Processing of Tasks, Desiognning notifications Systems using message queue/observer patterns), time based schedulers like crons jobs, load balancing algorithms, Data Paginiation and limtiing results, Search and Fileting, Acces contro and permissions, Handling concurrency and synchronization, File system organization (Searching)\n\nClasses and Methods\n\nClass Definition and Initialization\n\nClasses are the blueprint for creating objects in Python.\n\nclass MyClass:\n    def __init__(self, attribute1, attribute2):\n        self.attribute1 = attribute1  # Instance variable\n        self.attribute2 = attribute2\n\n# Creating an object\nobj = MyClass(\"value1\", \"value2\")\n\nInstance Variables and Methods\n\nInstance variables are specific to each object.\n\nInstance methods operate on instance variables.\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def greet(self):\n        return f\"Hello, my name is {self.name} and I am {self.age} years old.\"\n\n# Example usage\np = Person(\"Alice\", 25)\nprint(p.greet())  # Output: Hello, my name is Alice and I am 25 years old.\n\nClass Variables and Class Methods\n\nClass variables are shared among all instances of a class.\n\nClass methods operate on class variables and are denoted with @classmethod.\n\nclass Counter:\n    count = 0  # Class variable\n\n    @classmethod\n    def increment(cls):\n        cls.count += 1\n\n# Example usage\nCounter.increment()\nprint(Counter.count)  # Output: 1\n\nStatic Methods\n\nStatic methods do not operate on class or instance variables. They are utility functions within a class.\n\nclass MathUtils:\n    @staticmethod\n    def add(a, b):\n        return a + b\n\n# Example usage\nprint(MathUtils.add(5, 3))  # Output: 8\n\nOOP Concepts\n\nInheritance\n\nUse inheritance to create a hierarchy of classes.\n\nclass Animal:\n    def speak(self):\n        return \"I make a sound\"\n\nclass Dog(Animal):  # Dog inherits from Animal\n    def speak(self):\n        return \"Woof!\"\n\n# Example usage\ndog = Dog()\nprint(dog.speak())  # Output: Woof!\n\nEncapsulation\n\nUse private variables to restrict access to attributes.\n\nPrefix variables with _ (protected) or __ (private).\n\nclass BankAccount:\n    def __init__(self, balance):\n        self.__balance = balance  # Private variable\n\n    def get_balance(self):\n        return self.__balance\n\n# Example usage\naccount = BankAccount(100)\nprint(account.get_balance())  # Output: 100\n\nPolymorphism\n\nSame interface but different implementations.\n\nclass Animal:\n    def speak(self):\n        return \"Some sound\"\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\n# Example usage\nanimals = [Dog(), Cat()]\nfor animal in animals:\n    print(animal.speak())  # Output: Woof! Meow!\n\nAbstraction\n\nUse abstract classes to define methods that must be implemented in derived classes.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def area(self):\n        pass\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n# Example usage\nrect = Rectangle(5, 10)\nprint(rect.area())  # Output: 50\n\nSome Syntax Bits\n\nMethod Overloading (Default Arguments)\n\nPython doesn't support true method overloading, but you can simulate it using default arguments.\n\nclass Math:\n    def add(self, a, b=0, c=0):\n        return a + b + c\n\n# Example usage\nmath = Math()\nprint(math.add(5))        # Output: 5\nprint(math.add(5, 3))     # Output: 8\nprint(math.add(5, 3, 2))  # Output: 10\n\nMethod Overriding\n\nRedefine a method from the parent class in the child class.\n\nclass Parent:\n    def greet(self):\n        return \"Hello from Parent\"\n\nclass Child(Parent):\n    def greet(self):\n        return \"Hello from Child\"\n\n# Example usage\nchild = Child()\nprint(child.greet())  # Output: Hello from Child\n\nDesign Principles\n\n","favorited":false},{"name":"Random","id":"dd0b06c8-cc76-42e4-bba8-5fd595185473","color":"#999999","icon":"file-text","fileName":"Randomdd0b06c8-cc76-42e4-bba","textContent":"","favorited":false},{"name":"Applied Project","id":"6148ea0f-322d-4d54-99a2-cf83266ebbfa","color":"#999999","icon":"file-text","fileName":"Applied-Project6148ea0f-322d","textContent":"","favorited":false}],"opened":true}]}